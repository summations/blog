(dp1
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/content/posts/tech/multivariate-linearregression.md
p2
(F1492626679.5057995
(V<p>Linear regression with multiple variables is also known as "multivariate linear regression". Multiple linear regression is a generalization of linear regression by considering more than one independent variable, and a specific case of general linear models formed by restricting the number of dependent variables to one\u000aLets introduce notation for equations where we can have any number of input variables.</p>\u000a<div class="math">\u005cbegin{align*}\u000ax_j^{(i)} &amp;= \u005ctext{value of feature } j \u005ctext{ in the }i^{th}\u005ctext{ training set} \u005cnewline \u000ax^{(i)}&amp;= \u005ctext{column vector of all the features of }i^{th}\u005ctext{ training set} \u005cnewline \u000a&amp;m = \u005ctext{the number of training set} \u005cnewline \u000a&amp;n = \u005cleft| x^{(i)} \u005cright| ; \u005ctext{(the number of features)} \u000a\u005cend{align*}</div>\u000a<p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta (x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_2 + \u005ctheta_3 x_3 + \u005ccdots + \u005ctheta_n x_n\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>In order to develop intuition about this function, we can think about <span class="math">\u005c(\u005ctheta_0\u005c)</span> as the basic price of a house, <span class="math">\u005c(\u005ctheta_1\u005c)</span> as the price per square meter, <span class="math">\u005c(\u005ctheta_2\u005c)</span> as the price per floor, etc. <span class="math">\u005c(x_1\u005c)</span> will be the number of square meters in the house, <span class="math">\u005c(x_2\u005c)</span> the number of floors, etc.</p>\u000a<p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    h_\u005ctheta(x) =\u000a    \u005cbegin{bmatrix}\u000a        \u005ctheta_0 \u005chspace{2em} \u005ctheta_1 \u005chspace{2em} ... \u005chspace{2em} \u005ctheta_n\u000a    \u005cend{bmatrix}\u000a    \u005cbegin{bmatrix}\u000a        x_0 \u005cnewline \u000a        x_1 \u005cnewline \u000a        \u005cvdots \u005cnewline \u000a        x_n\u000a    \u005cend{bmatrix}= \u005ctheta^T x\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.</p>\u000a<p>Remark: Note that for convenience reasons in this course we assume <span class="math">\u005c(x_{0}^{(i)} =1 \u005ctext{ for } (i\u005cin { 1,\u005cdots, m } )\u005c)</span>. This allows us to do matrix operations with <span class="math">\u005c(\u005ctheta\u005c)</span> and <span class="math">\u005c(x\u005c)</span>. Hence making the two vectors <span class="math">\u005c(\u005ctheta\u005c)</span> and <span class="math">\u005c(x(i)\u005c)</span> match each other element-wise (that is, have the same number of elements: <span class="math">\u005c(n+1\u005c)</span>).</p>\u000a<p>The following example shows us the reason behind setting <span class="math">\u005c(x_{0}^{(i)}=1\u005c)</span> :</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    X = \u005cbegin{bmatrix}\u000a            x^{(1)}_0 &x^{(2)}_0 &x^{(3)}_0 \u005cnewline \u000a            x^{(1)}_1 &x^{(2)}_1 &x^{(3)}_1 \u000a        \u005cend{bmatrix} &,\u005ctheta = \u000a        \u005cbegin{bmatrix}\u000a            \u005ctheta_0 \u005cnewline \u000a            \u005ctheta_1 \u005cnewline\u000a        \u005cend{bmatrix}\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>As a result, you can calculate the hypothesis as a vector with <span class="math">\u005c(h_{\u03b8}(X)=\u03b8^TX\u005c)</span></p>\u000a<p><br/></p>\u000a<h4>Gradient Descent For Multiple Variables</h4>\u000a<p>The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*} \u000a\u005ctext{repeat until convergence:} \u005c; \u005clbrace \u005cnewline \u000a\u005c; &\u005ctheta_0 := \u005ctheta_0 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_0^{(i)} \u005cnewline \u000a\u005c; &\u005ctheta_1 := \u005ctheta_1 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_1^{(i)} \u005cnewline \u000a\u005c; &\u005ctheta_2 := \u005ctheta_2 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_2^{(i)} \u005cnewline \u000a&\u005ccdots \u005cnewline \u000a\u005crbrace \u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>In other words:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    \u005ctext{repeat until convergence:} \u005c; \u005clbrace \u005cnewline \u000a    \u005c; &\u005ctheta_j := \u005ctheta_j - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_j^{(i)} \u005c\u005c\u000a    \u005c; &\u005ctext{for j := 0...n}\u005cnewline \u000a    \u005crbrace\u000a\u005cend{align*}\u000a</div>\u000a\u000a<h3>Gradient Descent in Practice I - Feature Scaling</h3>\u000a<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because \u005c(\u005ctheta\u005c) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>\u000a<p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:</p>\u000a<div class="math">$$ \u22121 \u2264 x_i \u2264 1 $$</div>\u000a<p>or</p>\u000a<div class="math">$$ \u22120.5 \u2264 x_i \u2264 0.5 $$</div>\u000a<p>These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p>\u000a<p>Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    x_i &:=(x_i\u2212\u03bc_i)/s_i \u005cnewline\u000a    &s_i = \u005ctext{Standard Deviation} \u005cnewline\u000a    &\u03bc_i = \u005ctext{Mean}\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>Where \u03bci is the average of all the values for feature (i) and si is the range of values (max - min), or si is the standard deviation.</p>\u000a<p>Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.</p>\u000a<p>For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, xi:=price\u221210001900.</p>\u000a<h3>Gradient Descent in Practice II - Learning Rate</h3>\u000a<p>Note: [5:20 - the x -axis label in the right graph should be \u03b8 rather than No. of iterations ]</p>\u000a<p>Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(\u03b8) over the number of iterations of gradient descent. If J(\u03b8) ever increases, then you probably need to decrease \u03b1.</p>\u000a<p>Automatic convergence test. Declare convergence if J(\u03b8) decreases by less than E in one iteration, where E is some small value such as 10\u22123. However in practice it's difficult to choose this threshold value.</p>\u000a<div class="imgcap">\u000a<img alt="" src="/images/assets/lr-gd/costfunction-alpha1.png" style="border:none; width:50%;">\u000a</div>\u000a\u000a<p>It has been proven that if learning rate \u03b1 is sufficiently small, then \u005c( J(\u005ctheta)\u005c) will decrease on every iteration.</p>\u000a<div class="imgcap">\u000a<img alt= "" src="/images/assets/lr-gd/costfunction-alpha2.png" style="border:none; width:50%;">\u000a</div>\u000a\u000a<p>To summarize:\u000aIf \u03b1 is too small: slow convergence.\u000aIf \u03b1 is too large: \u005c( J(\u005ctheta)\u005c) may not decrease on every iteration and thus may not converge.</p>\u000a<h3>Features and Polynomial Regression</h3>\u000a<p>We can improve our features and the form of our hypothesis function in a couple different ways. We can combine multiple features into one. For example, we can combine <span class="math">\u005c(x_1\u005c)</span> and <span class="math">\u005c(x_2\u005c)</span> into a new feature <span class="math">\u005c(x_3\u005c)</span> by taking <span class="math">\u005c(x_1*x_2\u005c)</span>.</p>\u000a<h4>Polynomial Regression</h4>\u000a<p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well. We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). For example, if our hypothesis function is <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1\u005c)</span> then we can create additional features based on <span class="math">\u005c(x_1\u005c)</span> , to get the quadratic function <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_1^2\u005c)</span> or the cubic function <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_1^2 + \u005ctheta_3 x_1^3\u005c)</span>. </p>\u000a<p>In the cubic version, we have created new features <span class="math">\u005c(x_2\u005c)</span> and <span class="math">\u005c(x_3\u005c)</span> where <span class="math">\u005c(x_2=x_1^2\u005c)</span> and <span class="math">\u005c(x_3=x_1^3\u005c)</span>. To make it a square root function, we could do: <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 \u005csqrt{x_1}\u005c)</span>. One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important. eg. if <span class="math">\u005c(x_1\u005c)</span> has range 1 - 1000 then range of <span class="math">\u005c(x_1^2\u005c)</span> becomes 1 - 1000000 and that of <span class="math">\u005c(x_1^3\u005c)</span> becomes 1 - 1000000000</p>\u000a<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\u000a    var align = "center",\u000a        indent = "0em",\u000a        linebreak = "false";\u000a\u000a    if (false) {\u000a        align = (screen.width < 768) ? "left" : align;\u000a        indent = (screen.width < 768) ? "0em" : indent;\u000a        linebreak = (screen.width < 768) ? 'true' : linebreak;\u000a    }\u000a\u000a    var mathjaxscript = document.createElement('script');\u000a    var location_protocol = (false) ? 'https' : document.location.protocol;\u000a    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';\u000a    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\u000a    mathjaxscript.type = 'text/javascript';\u000a    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';\u000a    mathjaxscript[(window.opera ? "innerHTML" : "text")] =\u000a        "MathJax.Hub.Config({" +\u000a        "    config: ['MMLorHTML.js']," +\u000a        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +\u000a        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +\u000a        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +\u000a        "    displayAlign: '"+ align +"'," +\u000a        "    displayIndent: '"+ indent +"'," +\u000a        "    showMathMenu: true," +\u000a        "    messageStyle: 'normal'," +\u000a        "    tex2jax: { " +\u000a        "        inlineMath: [ ['\u005c\u005c\u005c\u005c(','\u005c\u005c\u005c\u005c)'] ], " +\u000a        "        displayMath: [ ['$$','$$'] ]," +\u000a        "        processEscapes: true," +\u000a        "        preview: 'TeX'," +\u000a        "    }, " +\u000a        "    'HTML-CSS': { " +\u000a        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +\u000a        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +\u000a        "    }, " +\u000a        "}); " +\u000a        "if ('default' !== 'default') {" +\u000a            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a        "}";\u000a    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\u000a}\u000a</script>
(dp3
Vdate
p4
cpelican.utils
SafeDatetime
p5
(S'\x07\xe1\x04\n\x12 \x00\x00\x00\x00'
tRp6
sVcategory
p7
ccopy_reg
_reconstructor
p8
(cpelican.urlwrappers
Category
p9
c__builtin__
object
p10
NtRp11
(dp12
S'_name'
p13
Vtech
p14
sS'_slug'
p15
Vtech
p16
sS'_slug_from_name'
p17
I01
sS'settings'
p18
(dp19
VCATEGORY_URL
p20
Vcategory/{slug}.html
p21
sVPAGE_ORDER_BY
p22
Vbasename
p23
sS'ARTICLE_TWEET_BUTTON'
p24
I00
sVWITH_FUTURE_DATES
p25
I01
sVCATEGORY_SAVE_AS
p26
Vcategory/{slug}.html
p27
sS'LINKS'
p28
((VCV/Resume
Vuser
V/pages/duncan-locks-resume.html
t(VLinkedIn
Vlinkedin-squared
Vhttp://ca.linkedin.com/in/duncanlock/
ttp29
sVARTICLE_ORDER_BY
p30
Vreversed-date
p31
sVDEFAULT_LANG
p32
Ven
p33
sVCACHE_CONTENT
p34
I01
sVPAGE_LANG_SAVE_AS
p35
Vpages/{slug}-{lang}.html
p36
sVDRAFT_SAVE_AS
p37
Vdrafts/{slug}.html
p38
sVPAGE_PATHS
p39
(lp40
Vpages
p41
asVDEFAULT_PAGINATION
p42
I8
sVDOCUTILS_SETTINGS
p43
(dp44
S'math_output'
p45
S'MathJax'
p46
ssVLOAD_CONTENT_CACHE
p47
I01
sVSTATIC_PATHS
p48
(lp49
Vimages
p50
aVextras
p51
aVfiles
p52
asVDAY_ARCHIVE_SAVE_AS
p53
Vblog/{date:%Y}/{date:%m}/{date:%d}/index.html
p54
sVARTICLE_LANG_URL
p55
V{slug}-{lang}.html
p56
sVSLUGIFY_SOURCE
p57
Vtitle
p58
sVSTATIC_URL
p59
V{path}
p60
sVCATEGORY_FEED_ATOM
p61
NsVOUTPUT_SOURCES
p62
I00
sVDIRECT_TEMPLATES
p63
(lp64
Vindex
p65
aVtags
p66
aVcategories
p67
aVauthors
p68
aVarchives
p69
asVSTATIC_EXCLUDES
p70
(lp71
sVTAG_URL
p72
Vtag/{slug}.html
p73
sVARTICLE_PATHS
p74
(lp75
Vposts
p76
asVLOG_FILTER
p77
(lp78
sVFILENAME_METADATA
p79
V(?P<date>\u005cd{4}-\u005cd{2}-\u005cd{2}).*
p80
sVDISPLAY_PAGES_ON_MENU
p81
I00
sVPELICAN_CLASS
p82
Vpelican.Pelican
p83
sVCACHE_PATH
p84
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/cache
p85
sVINDEX_SAVE_AS
p86
Vindex.html
p87
sVINTRASITE_LINK_REGEX
p88
V[{|](?P<what>.*?)[|}]
p89
sVDEFAULT_ORPHANS
p90
I1
sS'COLOPHON_TITLE'
p91
VAbout
p92
sS'ARCHIVES_SAVE_AS'
p93
Vblog/index.html
p94
sS'RESPONSIVE_IMAGES'
p95
I01
sS'AUTHOR'
p96
VRahul Pathak
p97
sS'MARKUP'
p98
(Vrst
Vhtml
tp99
sVDISPLAY_CATEGORIES_ON_MENU
p100
I01
sVCHECK_MODIFIED_METHOD
p101
Vmtime
p102
sVARTICLE_EXCLUDES
p103
(lp104
g41
asVTHEME_STATIC_PATHS
p105
(lp106
Vstatic
p107
asVTRANSLATION_FEED_ATOM
p108
NsVPLUGINS
p109
(lp110
Vassets
p111
aVrelated_posts
p112
aVextract_toc
p113
aVpost_stats
p114
aVseries
p115
aVrender_math
p116
aVsitemap
p117
asVTYPOGRIFY
p118
I01
sVFEED_ALL_ATOM
p119
NsS'TRANSLATION_FEED_RSS'
p120
NsVEXTRA_TEMPLATES_PATHS
p121
(lp122
sVMARKDOWN
p123
(dp124
Vextension_configs
p125
(dp126
Vmarkdown.extensions.extra
p127
(dp128
sVmarkdown.extensions.codehilite
p129
(dp130
Vcss_class
p131
Vhighlight
p132
ssVmarkdown.extensions.meta
p133
(dp134
ssS'extensions'
p135
(lp136
g8
(crender_math.pelican_mathjax_markdown_extension
PelicanMathJaxExtension
p137
g10
NtRp138
(dp139
S'mathjax_needed'
p140
I00
sbag127
ag129
ag133
asVoutput_format
p141
Vhtml5
p142
ssVOUTPUT_RETENTION
p143
(lp144
sS'COLOPHON'
p145
I01
sS'PROFILE_IMAGE'
p146
Vsigma-black.svg
p147
sVPAGE_LANG_URL
p148
Vpages/{slug}-{lang}.html
p149
sVTHEME_STATIC_DIR
p150
Vtheme
p151
sVTHEME
p152
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/blueprint
p153
sVUSE_FOLDER_AS_CATEGORY
p154
I01
sVJINJA_ENVIRONMENT
p155
(dp156
Vextensions
p157
(lp158
cwebassets.ext.jinja2
AssetsExtension
p159
asVtrim_blocks
p160
I01
sVlstrip_blocks
p161
I01
ssVDEBUG
p162
I01
sVREVERSE_CATEGORY_ORDER
p163
I00
sVRELATIVE_URLS
p164
I00
sVDATE_FORMATS
p165
(dp166
sVSITEURL
p167
Vhttp://localhost:8000
p168
sVCONTENT_CACHING_LAYER
p169
Vreader
p170
sS'SOCIAL'
p171
((VGitHub
Vgithub-circled
Vhttp://github.com/rahulpathakml
ttp172
sVNEWEST_FIRST_ARCHIVES
p173
I01
sS'SITE_TITLE'
p174
VRahul Pathak Blog
p175
sS'COPYRIGHT_UNTIL'
p176
I2017
sVPATH_METADATA
p177
V
sVTEMPLATE_PAGES
p178
(dp179
sVFEED_DOMAIN
p180
g168
sVTYPOGRIFY_IGNORE_TAGS
p181
(lp182
S'.math'
p183
aS'script'
p184
asVGZIP_CACHE
p185
I00
sS'SITE_DESCRIPTION'
p186
VRahul Pathak personal site. Includes my blog, colitis resources, SCD recipes, portfolio and CV/Resume.
p187
sVAUTHOR_FEED_RSS
p188
Vfeeds/%s.rss.xml
p189
sS'TIMEZONE'
p190
VAmerica/Vancouver
p191
sVDRAFT_URL
p192
Vdrafts/{slug}.html
p193
sVPAGE_EXCLUDES
p194
(lp195
g76
asVAUTHOR_SAVE_AS
p196
I00
sVREADERS
p197
(dp198
sVDRAFT_LANG_URL
p199
Vdrafts/{slug}-{lang}.html
p200
sVOUTPUT_SOURCES_EXTENSION
p201
V.text
p202
sVPAGINATION_PATTERNS
p203
(lp204
g8
(cpelican.paginator
PaginationRule
p205
c__builtin__
tuple
p206
(I0
V{name}{number}{extension}
p207
V{name}{number}{extension}
p208
ttRp209
asVSTATIC_EXCLUDE_SOURCES
p210
I01
sVJINJA_FILTERS
p211
(dp212
Vmonth_name
p213
cpelicanconf
month_name
p214
sVtagsort
p215
cpelicanconf
tagsort
p216
sVsidebar_date_format
p217
cpelicanconf
sidebar_date_format
p218
sVarchive_date_format
p219
cpelicanconf
archive_date_format
p220
sVdump
p221
cpelicanconf
dump
p222
ssVARTICLE_SAVE_AS
p223
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/index.html
p224
sVYEAR_ARCHIVE_SAVE_AS
p225
Vblog/{date:%Y}/index.html
p226
sVCSS_FILE
p227
Vmain.css
p228
sVAUTHOR_URL
p229
Vauthor/{slug}.html
p230
sVIGNORE_FILES
p231
(lp232
V.#*
p233
asS'SITEMAP'
p234
(dp235
Vchangefreqs
p236
(dp237
Varticles
p238
Vweekly
p239
sg41
Vmonthly
p240
sVindexes
p241
Vdaily
p242
ssVpriorities
p243
(dp244
g238
F0.80000000000000004
sg41
F0.5
sg241
F0.69999999999999996
ssVformat
p245
Vxml
p246
ssVSUMMARY_MAX_LENGTH
p247
I60
sVSLUG_SUBSTITUTIONS
p248
(tsS'RELATED_POSTS_MAX'
p249
I4
sVFORMATTED_FIELDS
p250
(lp251
Vsummary
p252
asVDEFAULT_DATE_FORMAT
p253
V%a %d %B %Y
p254
sS'CATEGORY_FEED_RSS'
p255
NsS'DEFAULT_PAGESCHEMA'
p256
VBlogPost
p257
sVDRAFT_LANG_SAVE_AS
p258
Vdrafts/{slug}-{lang}.html
p259
sS'FEED_ALL_RSS'
p260
NsVEXTRA_PATH_METADATA
p261
(dp262
Vextras/.htaccess
p263
(dp264
Vpath
p265
V.htaccess
p266
ssVextras/google9b8f9c7f2338fb3e.html
p267
(dp268
g265
Vgoogle9b8f9c7f2338fb3e.html
p269
ssVextras/favicon.ico
p270
(dp271
g265
Vfavicon.ico
p272
ssVextras/robots.txt
p273
(dp274
g265
Vrobots.txt
p275
sssVLOCALE
p276
(lp277
V
asVWRITE_SELECTED
p278
(lp279
sVDELETE_OUTPUT_DIRECTORY
p280
I00
sVFEED_MAX_ITEMS
p281
V
sVPATH
p282
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/content
p283
sVPAGE_URL
p284
Vpages/{slug}.html
p285
sVSITENAME
p286
VSummations
p287
sVARTICLE_PERMALINK_STRUCTURE
p288
V
sS'COLOPHON_CONTENT'
p289
V<a href="/pages/duncan-locks-resume.html">\u000aAn adaptable and enthusiastic developer with broad experience\u000aand an artistic back&shy;ground. Strong graph&shy;ical\u000acom&shy;munication, design, creative\u000a<span class="amp">&amp;</span> prob&shy;lem solving\u000askills &mdash; and an eye for detail.</a>
p290
sVARTICLE_URL
p291
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/
p292
sS'SITESCHEME'
p293
Vhttps
p294
sVMONTH_ARCHIVE_SAVE_AS
p295
Vblog/{date:%Y}/{date:%m}/index.html
p296
sS'ARTICLE_GOOGLEPLUS_BUTTON'
p297
I00
sVDEFAULT_METADATA
p298
(dp299
sVRSS_FEED_SUMMARY_ONLY
p300
I01
sVDEFAULT_CATEGORY
p301
Vmisc
p302
sVPLUGIN_PATHS
p303
(lp304
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/pelican-plugins
p305
asVPAGINATED_DIRECT_TEMPLATES
p306
(lp307
g65
asVSTATIC_SAVE_AS
p308
g60
sVTAG_SAVE_AS
p309
Vtag/{slug}.html
p310
sVPYGMENTS_RST_OPTIONS
p311
(dp312
sVAUTHOR_FEED_ATOM
p313
Vfeeds/%s.atom.xml
p314
sVARTICLE_LANG_SAVE_AS
p315
g56
sVPAGE_SAVE_AS
p316
Vpages/{slug}.html
p317
sS'OPEN_GRAPH_METADATA'
p318
I01
sVOUTPUT_PATH
p319
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/output
p320
sVDEFAULT_STATUS
p321
Vpublished
p322
sS'DISQUS_SITENAME'
p323
Vrahulpathakml
p324
sS'COPYRIGHT_FROM'
p325
I1998
ssbsVtitle
p326
VUnderstanding Multivariate Linear Regression
p327
sVtags
p328
(lp329
g8
(cpelican.urlwrappers
Tag
p330
g10
NtRp331
(dp332
g13
Vmachinelearning
p333
sg15
Vmachinelearning
p334
sg17
I01
sg18
g19
sbag8
(g330
g10
NtRp335
(dp336
g13
Vregression
p337
sg15
Vregression
p338
sg17
I01
sg18
g19
sbasttp339
sV/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/content/posts/tech/Logistic Regression.md
p340
(F1492609690.0344877
(V<p>To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn't work well because classification is not actually a linear function.</p>\u000a<p>The classification problem is just like the regression problem, except that the values <em>y</em> we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then <span class="math">\u005c(x^{(i)}\u005c)</span> may be some features of a piece of email, and <span class="math">\u005c(y\u005c)</span> may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, <span class="math">\u005c(y\u005cin{0,1}\u005c)</span>. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols \u201c-\u201d and \u201c+.\u201d Given x(i), the corresponding <span class="math">\u005c(y^{(i)}\u005c)</span> is also called the label for the training example. </p>\u000a<h3>Hypothesis Representation</h3>\u000a<p>We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict <span class="math">\u005c(y\u005c)</span> given <span class="math">\u005c(x\u005c)</span>. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn\u2019t make sense for <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> to take values larger than 1 or smaller than 0 when we know that <span class="math">\u005c(y \u2208 {0, 1}\u005c)</span>. To fix this, let\u2019s change the form for our hypotheses <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> to satisfy <span class="math">\u005c(0\u2264h_\u005ctheta(x)\u22641\u005c)</span>. This is accomplished by plugging <span class="math">\u005c(\u005ctheta^Tx\u005c)</span> into the Logistic Function.</p>\u000a<p>Our new form uses the "Sigmoid Function," also called the "Logistic Function":</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)=g(\u005ctheta^Tx)z=\u005ctheta^Txg(z)=11+e\u2212z\u000a\u005cend{align*}\u000a</div>\u000a\u000a<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">loadtxt</span><span class="p">,</span> <span class="n">where</span>\u000a<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">scatter</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">legend</span><span class="p">,</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span>\u000a\u000a<span class="c1">#load the dataset</span>\u000a<span class="n">data</span> <span class="o">=</span> <span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;ex2data1.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>\u000a\u000a<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>\u000a<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>\u000a\u000a<span class="n">pos</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>\u000a<span class="n">neg</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>\u000a<span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>\u000a<span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>\u000a<span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>\u000a<span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>\u000a<span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Not Admitted&#39;</span><span class="p">,</span> <span class="s1">&#39;Admitted&#39;</span><span class="p">])</span>\u000a<span class="n">show</span><span class="p">()</span>\u000a</pre></div>\u000a\u000a\u000a<p>The following image shows us what the sigmoid function looks like:</p>\u000a<div class="figure">\u000a<img alt="" src="/images/assets/logisticreg/sigmoidfunction.png" style="border:none; width:100%;">\u000a</div>\u000a\u000a<p>The function <span class="math">\u005c(g(z)\u005c)</span>, shown here, maps any real number to the <span class="math">\u005c((0, 1)\u005c)</span> interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.</p>\u000a<p><span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> will give us the probability that our output is 1. For example, <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>=0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)=P(y=1|x;\u005ctheta)=1\u2212P(y=0|x;\u005ctheta)P(y=0|x;\u005ctheta)+P(y=1|x;\u005ctheta)=1\u000a\u005cend{align*}\u000a</div>\u000a\u000a<h3>Decision Boundary</h3>\u000a<p>In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)\u22650.5\u2192y=1 \u005cnewline\u000ah_\u005ctheta(x) < 0.5\u2192y=0\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5: that is when <span class="math">\u005c(g(z)\u22650.5\u005c)</span> and when <span class="math">\u005c(z\u22650\u005c)</span></p>\u000a<p>Remember.</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000az=0, e_0=1&\u21d2g(z)=1/2 \u005cnewline\u000az\u2192\u221e, e^{\u2212\u221e}\u21920&\u21d2g(z)=1 \u005cnewline\u000az\u2192\u2212\u221e, e^{\u221e}\u2192\u221e&\u21d2g(z)=0  \u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>So if our input to g is <span class="math">\u005c(\u005ctheta^TX\u005c)</span>, then that means: <span class="math">\u005c(h_\u005ctheta(x)=g(\u005ctheta^Tx)\u22650.5\u005c)</span> when <span class="math">\u005c(\u005ctheta^Tx\u22650\u005c)</span>.\u000aFrom these statements we can now say: <span class="math">\u005c(\u005ctheta^Tx\u22650\u21d2y=1\u005c)</span> and <span class="math">\u005c(\u005ctheta^Tx&lt;0\u21d2y=0\u005c)</span>.\u000aThe decision boundary is the line that separates the area where <span class="math">\u005c(y = 0\u005c)</span> and where <span class="math">\u005c(y = 1\u005c)</span>. It is created by our hypothesis function.</p>\u000a<p>Example:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align}\u000a    \u005ctheta = \u000a    &\u005cbegin{bmatrix}\u000a            5  \u005cnewline\u000a            -1 \u005cnewline\u000a            0  \u005cnewline\u000a    \u005cend{bmatrix} \u005cnewline\u000a    y = 1 \u005cquad &if \u005cquad 5+(\u22121)x_1+0x_2\u22650 \u005cnewline\u000a    5\u2212x_1 &\u2265 0 \u005cnewline\u000a    \u2212x_1 &\u2265 \u22125 \u005cnewline\u000a    x_1 &\u2264 5 \u005cnewline\u000a\u005cend{align}\u000a</div>\u000a\u000a<p>In this case, our decision boundary is a straight vertical line placed on the graph where x1=5, and everything to the left of that denotes <span class="math">\u005c(y\u005c)</span> = 1, while everything to the right denotes <span class="math">\u005c(y\u005c)</span> = 0.</p>\u000a<p>Again, the input to the sigmoid function g(z) (e.g. <span class="math">\u005c(\u005ctheta^TX\u005c)</span>) doesn't need to be linear, and could be a function that describes a circle (e.g. <span class="math">\u005c(z=\u005ctheta_0+\u005ctheta_1x^{2}_1+\u005ctheta_2x^{2}_2\u005c)</span>) or any shape to fit our data.</p>\u000a<h3>Cost Function</h3>\u000a<p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p>\u000a<p>Instead, our cost function for logistic regression looks like:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    J(\u005ctheta) &= \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} Cost(h_\u005ctheta(x^{(i)}),y^{(i)}) \u005cnewline\u000a    Cost(h_\u005ctheta(x),y) &= \u2212log\u2061(h_\u005ctheta(x)) \u005cquad if \u005c; y = 1 \u005cnewline\u000a    Cost(h_\u005ctheta(x),y) &= \u2212log\u2061(1\u2212h_\u005ctheta(x)) \u005cquad if \u005c; y = 0 \u005cnewline\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>When y = 1, we get the following plot for <span class="math">\u005c(J(\u005ctheta)\u005c)</span> vs <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>:\u000a<div class="imgcap">\u000a<img alt="/images/assets/logisticreg/costvshypo1.png" src="/images/assets/logisticreg/costvshypo1.png" style="border:none; width:30%;">\u000a</div></p>\u000a<p>Similarly, when y = 0, we get the following plot for <span class="math">\u005c(J(\u005ctheta)\u005c)</span> vs <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>:</p>\u000a<div class="imgcap">\u000a<img alt="/images//assets/logisticreg/costvshypo2.png" src="/images//assets/logisticreg/costvshypo2.png" style="border:none; width:30%;">\u000a</div>\u000a\u000a<p>Now our cost function becomes:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    Cost(h_\u005ctheta(x),y)&=0 \u005cquad &if \u005c; h_\u005ctheta(x)=y \u005cnewline\u000a    Cost(h_\u005ctheta(x),y)&\u2192\u221e \u005cquad &if \u005c; y=0 \u005c; and \u005c; h_\u005ctheta(x)\u21921 \u005cnewline\u000a    Cost(h_\u005ctheta(x),y)&\u2192\u221e \u005cquad &if \u005c; y=1 \u005c; and \u005c; h_\u005ctheta(x)\u21920 \u005cnewline\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>If our correct answer <span class="math">\u005c(y\u005c)</span> is <span class="math">\u005c(0\u005c)</span>, then the cost function will be <span class="math">\u005c(0\u005c)</span> if our hypothesis function also outputs <span class="math">\u005c(0\u005c)</span>. If our hypothesis approaches <span class="math">\u005c(1\u005c)</span>, then the cost function will approach infinity.</p>\u000a<p>If our correct answer <span class="math">\u005c(y\u005c)</span> is <span class="math">\u005c(1\u005c)</span>, then the cost function will be <span class="math">\u005c(0\u005c)</span> if our hypothesis function outputs <span class="math">\u005c(1\u005c)</span>. If our hypothesis approaches <span class="math">\u005c(0\u005c)</span>, then the cost function will approach infinity.</p>\u000a<p>Note that writing the cost function in this way guarantees that <span class="math">\u005c(J(\u005ctheta)\u005c)</span> is convex for logistic regression.</p>\u000a<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\u000a    var align = "center",\u000a        indent = "0em",\u000a        linebreak = "false";\u000a\u000a    if (false) {\u000a        align = (screen.width < 768) ? "left" : align;\u000a        indent = (screen.width < 768) ? "0em" : indent;\u000a        linebreak = (screen.width < 768) ? 'true' : linebreak;\u000a    }\u000a\u000a    var mathjaxscript = document.createElement('script');\u000a    var location_protocol = (false) ? 'https' : document.location.protocol;\u000a    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';\u000a    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\u000a    mathjaxscript.type = 'text/javascript';\u000a    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';\u000a    mathjaxscript[(window.opera ? "innerHTML" : "text")] =\u000a        "MathJax.Hub.Config({" +\u000a        "    config: ['MMLorHTML.js']," +\u000a        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +\u000a        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +\u000a        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +\u000a        "    displayAlign: '"+ align +"'," +\u000a        "    displayIndent: '"+ indent +"'," +\u000a        "    showMathMenu: true," +\u000a        "    messageStyle: 'normal'," +\u000a        "    tex2jax: { " +\u000a        "        inlineMath: [ ['\u005c\u005c\u005c\u005c(','\u005c\u005c\u005c\u005c)'] ], " +\u000a        "        displayMath: [ ['$$','$$'] ]," +\u000a        "        processEscapes: true," +\u000a        "        preview: 'TeX'," +\u000a        "    }, " +\u000a        "    'HTML-CSS': { " +\u000a        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +\u000a        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +\u000a        "    }, " +\u000a        "}); " +\u000a        "if ('default' !== 'default') {" +\u000a            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a        "}";\u000a    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\u000a}\u000a</script>
(dp341
Vdate
p342
g5
(S'\x07\xe1\x04\x0b\x12 \x00\x00\x00\x00'
tRp343
sVcategory
p344
g8
(g9
g10
NtRp345
(dp346
g13
Vtech
p347
sg15
Vtech
p348
sg17
I01
sg18
(dp349
VIGNORE_FILES
p350
(lp351
V.#*
p352
asVPAGE_ORDER_BY
p353
Vbasename
p354
sS'ARTICLE_TWEET_BUTTON'
p355
I00
sVWITH_FUTURE_DATES
p356
I01
sVCATEGORY_SAVE_AS
p357
Vcategory/{slug}.html
p358
sS'LINKS'
p359
((VCV/Resume
Vuser
V/pages/duncan-locks-resume.html
t(VLinkedIn
Vlinkedin-squared
Vhttp://ca.linkedin.com/in/duncanlock/
ttp360
sVARTICLE_ORDER_BY
p361
Vreversed-date
p362
sVDEFAULT_LANG
p363
Ven
p364
sVCACHE_CONTENT
p365
I01
sVPAGE_LANG_SAVE_AS
p366
Vpages/{slug}-{lang}.html
p367
sVDRAFT_SAVE_AS
p368
Vdrafts/{slug}.html
p369
sVPAGE_PATHS
p370
(lp371
Vpages
p372
asVDEFAULT_PAGINATION
p373
I8
sVDOCUTILS_SETTINGS
p374
(dp375
S'math_output'
p376
S'MathJax'
p377
ssVLOAD_CONTENT_CACHE
p378
I01
sVSTATIC_PATHS
p379
(lp380
Vimages
p381
aVextras
p382
aVfiles
p383
asVDAY_ARCHIVE_SAVE_AS
p384
Vblog/{date:%Y}/{date:%m}/{date:%d}/index.html
p385
sVARTICLE_LANG_URL
p386
V{slug}-{lang}.html
p387
sVSLUGIFY_SOURCE
p388
Vtitle
p389
sVSTATIC_URL
p390
V{path}
p391
sVCATEGORY_FEED_ATOM
p392
NsVOUTPUT_SOURCES
p393
I00
sVPAGINATED_DIRECT_TEMPLATES
p394
(lp395
Vindex
p396
asVTYPOGRIFY_IGNORE_TAGS
p397
(lp398
S'.math'
p399
aS'script'
p400
asVSTATIC_EXCLUDES
p401
(lp402
sVTAG_URL
p403
Vtag/{slug}.html
p404
sVARTICLE_PATHS
p405
(lp406
Vposts
p407
asVGZIP_CACHE
p408
I00
sVFILENAME_METADATA
p409
V(?P<date>\u005cd{4}-\u005cd{2}-\u005cd{2}).*
p410
sVDEFAULT_ORPHANS
p411
I1
sVPELICAN_CLASS
p412
Vpelican.Pelican
p413
sVCACHE_PATH
p414
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/cache
p415
sVINDEX_SAVE_AS
p416
Vindex.html
p417
sVINTRASITE_LINK_REGEX
p418
V[{|](?P<what>.*?)[|}]
p419
sVDISPLAY_PAGES_ON_MENU
p420
I00
sS'COLOPHON_TITLE'
p421
VAbout
p422
sS'ARCHIVES_SAVE_AS'
p423
Vblog/index.html
p424
sS'RESPONSIVE_IMAGES'
p425
I01
sS'AUTHOR'
p426
VRahul Pathak
p427
sS'MARKUP'
p428
(Vrst
Vhtml
tp429
sVDISPLAY_CATEGORIES_ON_MENU
p430
I01
sVCHECK_MODIFIED_METHOD
p431
Vmtime
p432
sVARTICLE_EXCLUDES
p433
(lp434
g372
asVDEFAULT_DATE_FORMAT
p435
V%a %d %B %Y
p436
sVTRANSLATION_FEED_ATOM
p437
NsVPLUGINS
p438
(lp439
Vassets
p440
aVrelated_posts
p441
aVextract_toc
p442
aVpost_stats
p443
aVseries
p444
aVrender_math
p445
aVsitemap
p446
asVTYPOGRIFY
p447
I01
sS'TRANSLATION_FEED_RSS'
p448
NsVEXTRA_TEMPLATES_PATHS
p449
(lp450
sVMARKDOWN
p451
(dp452
Vextension_configs
p453
(dp454
Vmarkdown.extensions.extra
p455
(dp456
sVmarkdown.extensions.codehilite
p457
(dp458
Vcss_class
p459
Vhighlight
p460
ssVmarkdown.extensions.meta
p461
(dp462
ssS'extensions'
p463
(lp464
g8
(g137
g10
NtRp465
(dp466
g140
I00
sbag455
ag457
ag461
asVoutput_format
p467
Vhtml5
p468
ssVOUTPUT_RETENTION
p469
(lp470
sS'COLOPHON'
p471
I01
sVPAGE_LANG_URL
p472
Vpages/{slug}-{lang}.html
p473
sS'DEFAULT_PAGESCHEMA'
p474
VBlogPost
p475
sVTHEME
p476
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/blueprint
p477
sVUSE_FOLDER_AS_CATEGORY
p478
I01
sVJINJA_ENVIRONMENT
p479
(dp480
Vextensions
p481
(lp482
g159
asVlstrip_blocks
p483
I01
sVtrim_blocks
p484
I01
ssVDEBUG
p485
I01
sVREVERSE_CATEGORY_ORDER
p486
I00
sVRELATIVE_URLS
p487
I00
sVDATE_FORMATS
p488
(dp489
sVSITEURL
p490
Vhttps://summations.github.io
p491
sVCONTENT_CACHING_LAYER
p492
Vreader
p493
sS'SOCIAL'
p494
((VGitHub
Vgithub-circled
Vhttp://github.com/rahulpathakml
ttp495
sVNEWEST_FIRST_ARCHIVES
p496
I01
sS'SITE_TITLE'
p497
VRahul Pathak Blog
p498
sS'COPYRIGHT_UNTIL'
p499
I2017
sVPATH_METADATA
p500
V
sVTEMPLATE_PAGES
p501
(dp502
sVFEED_DOMAIN
p503
g491
sVDIRECT_TEMPLATES
p504
(lp505
g396
aVtags
p506
aVcategories
p507
aVauthors
p508
aVarchives
p509
asVLOG_FILTER
p510
(lp511
sS'SITE_DESCRIPTION'
p512
VRahul Pathak personal site. Includes my blog, colitis resources, SCD recipes, portfolio and CV/Resume.
p513
sVAUTHOR_FEED_RSS
p514
Vfeeds/%s.rss.xml
p515
sS'TIMEZONE'
p516
VAmerica/Vancouver
p517
sVDRAFT_URL
p518
Vdrafts/{slug}.html
p519
sVPAGE_EXCLUDES
p520
(lp521
g407
asVTAG_SAVE_AS
p522
Vtag/{slug}.html
p523
sVEXTRA_PATH_METADATA
p524
(dp525
Vextras/.htaccess
p526
(dp527
Vpath
p528
V.htaccess
p529
ssVextras/google9b8f9c7f2338fb3e.html
p530
(dp531
g528
Vgoogle9b8f9c7f2338fb3e.html
p532
ssVextras/favicon.ico
p533
(dp534
g528
Vfavicon.ico
p535
ssVextras/robots.txt
p536
(dp537
g528
Vrobots.txt
p538
sssVDEFAULT_CATEGORY
p539
Vmisc
p540
sVDRAFT_LANG_URL
p541
Vdrafts/{slug}-{lang}.html
p542
sVOUTPUT_SOURCES_EXTENSION
p543
V.text
p544
sVPAGINATION_PATTERNS
p545
(lp546
g8
(g205
g206
(I0
V{name}{number}{extension}
p547
V{name}{number}{extension}
p548
ttRp549
asVSTATIC_EXCLUDE_SOURCES
p550
I01
sVJINJA_FILTERS
p551
(dp552
Vmonth_name
p553
g214
sVtagsort
p554
g216
sVsidebar_date_format
p555
g218
sVarchive_date_format
p556
g220
sVdump
p557
g222
ssVARTICLE_SAVE_AS
p558
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/index.html
p559
sVYEAR_ARCHIVE_SAVE_AS
p560
Vblog/{date:%Y}/index.html
p561
sVCSS_FILE
p562
Vmain.css
p563
sVAUTHOR_URL
p564
Vauthor/{slug}.html
p565
sVCATEGORY_URL
p566
Vcategory/{slug}.html
p567
sS'SITEMAP'
p568
(dp569
Vchangefreqs
p570
(dp571
Varticles
p572
Vweekly
p573
sg372
Vmonthly
p574
sVindexes
p575
Vdaily
p576
ssVpriorities
p577
(dp578
g572
F0.80000000000000004
sg372
F0.5
sg575
F0.69999999999999996
ssVformat
p579
Vxml
p580
ssVSUMMARY_MAX_LENGTH
p581
I60
sVSLUG_SUBSTITUTIONS
p582
(tsS'RELATED_POSTS_MAX'
p583
I4
sVFORMATTED_FIELDS
p584
(lp585
Vsummary
p586
asVTHEME_STATIC_PATHS
p587
(lp588
Vstatic
p589
asS'CATEGORY_FEED_RSS'
p590
NsVTHEME_STATIC_DIR
p591
Vtheme
p592
sVDRAFT_LANG_SAVE_AS
p593
Vdrafts/{slug}-{lang}.html
p594
sS'FEED_ALL_RSS'
p595
NsVREADERS
p596
(dp597
sVLOCALE
p598
(lp599
V
asVWRITE_SELECTED
p600
(lp601
sVDELETE_OUTPUT_DIRECTORY
p602
I00
sVFEED_MAX_ITEMS
p603
V
sVPATH
p604
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/content
p605
sVPAGE_URL
p606
Vpages/{slug}.html
p607
sVSITENAME
p608
VSummations
p609
sVARTICLE_PERMALINK_STRUCTURE
p610
V
sS'COLOPHON_CONTENT'
p611
V<a href="/pages/duncan-locks-resume.html">\u000aAn adaptable and enthusiastic developer with broad experience\u000aand an artistic back&shy;ground. Strong graph&shy;ical\u000acom&shy;munication, design, creative\u000a<span class="amp">&amp;</span> prob&shy;lem solving\u000askills &mdash; and an eye for detail.</a>
p612
sVARTICLE_URL
p613
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/
p614
sS'SITESCHEME'
p615
Vhttps
p616
sVMONTH_ARCHIVE_SAVE_AS
p617
Vblog/{date:%Y}/{date:%m}/index.html
p618
sVDEFAULT_METADATA
p619
(dp620
sVRSS_FEED_SUMMARY_ONLY
p621
I01
sS'ARTICLE_GOOGLEPLUS_BUTTON'
p622
I00
sVPLUGIN_PATHS
p623
(lp624
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/pelican-plugins
p625
asVFEED_ALL_ATOM
p626
NsVSTATIC_SAVE_AS
p627
g391
sVAUTHOR_SAVE_AS
p628
I00
sVPYGMENTS_RST_OPTIONS
p629
(dp630
sVAUTHOR_FEED_ATOM
p631
Vfeeds/%s.atom.xml
p632
sVARTICLE_LANG_SAVE_AS
p633
g387
sVPAGE_SAVE_AS
p634
Vpages/{slug}.html
p635
sS'OPEN_GRAPH_METADATA'
p636
I01
sVOUTPUT_PATH
p637
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/pelicanboostrap/output
p638
sVDEFAULT_STATUS
p639
Vpublished
p640
sS'DISQUS_SITENAME'
p641
Vrahulpathakml
p642
sS'COPYRIGHT_FROM'
p643
I1998
ssbsVtags
p644
(lp645
g8
(g330
g10
NtRp646
(dp647
g13
Vmachinelearning
p648
sg15
Vmachinelearning
p649
sg17
I01
sg18
g349
sbag8
(g330
g10
NtRp650
(dp651
g13
Vclassification
p652
sg15
Vclassification
p653
sg17
I01
sg18
g349
sbasVtitle
p654
VLogistic Regression
p655
sttp656
sV/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blogsrc/content/posts/tech/Logistic Regression.md
p657
(F1492255010.7084544
(V<p>To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn't work well because classification is not actually a linear function.</p>\u000a<p>The classification problem is just like the regression problem, except that the values <em>y</em> we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then <span class="math">\u005c(x^{(i)}\u005c)</span> may be some features of a piece of email, and <span class="math">\u005c(y\u005c)</span> may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, <span class="math">\u005c(y\u005cin{0,1}\u005c)</span>. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols \u201c-\u201d and \u201c+.\u201d Given x(i), the corresponding <span class="math">\u005c(y^{(i)}\u005c)</span> is also called the label for the training example. </p>\u000a<h3>Hypothesis Representation</h3>\u000a<p>We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict <span class="math">\u005c(y\u005c)</span> given <span class="math">\u005c(x\u005c)</span>. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn\u2019t make sense for <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> to take values larger than 1 or smaller than 0 when we know that <span class="math">\u005c(y \u2208 {0, 1}\u005c)</span>. To fix this, let\u2019s change the form for our hypotheses <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> to satisfy <span class="math">\u005c(0\u2264h_\u005ctheta(x)\u22641\u005c)</span>. This is accomplished by plugging <span class="math">\u005c(\u005ctheta^Tx\u005c)</span> into the Logistic Function.</p>\u000a<p>Our new form uses the "Sigmoid Function," also called the "Logistic Function":</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)=g(\u005ctheta^Tx)z=\u005ctheta^Txg(z)=11+e\u2212z\u000a\u005cend{align*}\u000a</div>\u000a\u000a<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">loadtxt</span><span class="p">,</span> <span class="n">where</span>\u000a<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">scatter</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">legend</span><span class="p">,</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span>\u000a\u000a<span class="c1">#load the dataset</span>\u000a<span class="n">data</span> <span class="o">=</span> <span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;ex2data1.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>\u000a\u000a<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>\u000a<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>\u000a\u000a<span class="n">pos</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>\u000a<span class="n">neg</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>\u000a<span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>\u000a<span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>\u000a<span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>\u000a<span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>\u000a<span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Not Admitted&#39;</span><span class="p">,</span> <span class="s1">&#39;Admitted&#39;</span><span class="p">])</span>\u000a<span class="n">show</span><span class="p">()</span>\u000a</pre></div>\u000a\u000a\u000a<p>The following image shows us what the sigmoid function looks like:</p>\u000a<div class="figure">\u000a<img alt="" src="/images/assets/logisticreg/sigmoidfunction.png" style="border:none; width:100%;">\u000a</div>\u000a\u000a<p>The function <span class="math">\u005c(g(z)\u005c)</span>, shown here, maps any real number to the <span class="math">\u005c((0, 1)\u005c)</span> interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.</p>\u000a<p><span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> will give us the probability that our output is 1. For example, <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>=0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)=P(y=1|x;\u005ctheta)=1\u2212P(y=0|x;\u005ctheta)P(y=0|x;\u005ctheta)+P(y=1|x;\u005ctheta)=1\u000a\u005cend{align*}\u000a</div>\u000a\u000a<h3>Decision Boundary</h3>\u000a<p>In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)\u22650.5\u2192y=1 \u005cnewline\u000ah_\u005ctheta(x) < 0.5\u2192y=0\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5: that is when <span class="math">\u005c(g(z)\u22650.5\u005c)</span> and when <span class="math">\u005c(z\u22650\u005c)</span></p>\u000a<p>Remember.</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000az=0, e_0=1&\u21d2g(z)=1/2 \u005cnewline\u000az\u2192\u221e, e^{\u2212\u221e}\u21920&\u21d2g(z)=1 \u005cnewline\u000az\u2192\u2212\u221e, e^{\u221e}\u2192\u221e&\u21d2g(z)=0  \u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>So if our input to g is <span class="math">\u005c(\u005ctheta^TX\u005c)</span>, then that means: <span class="math">\u005c(h_\u005ctheta(x)=g(\u005ctheta^Tx)\u22650.5\u005c)</span> when <span class="math">\u005c(\u005ctheta^Tx\u22650\u005c)</span>.\u000aFrom these statements we can now say: <span class="math">\u005c(\u005ctheta^Tx\u22650\u21d2y=1\u005c)</span> and <span class="math">\u005c(\u005ctheta^Tx&lt;0\u21d2y=0\u005c)</span>.\u000aThe decision boundary is the line that separates the area where <span class="math">\u005c(y = 0\u005c)</span> and where <span class="math">\u005c(y = 1\u005c)</span>. It is created by our hypothesis function.</p>\u000a<p>Example:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align}\u000a    \u005ctheta = \u000a    &\u005cbegin{bmatrix}\u000a            5  \u005cnewline\u000a            -1 \u005cnewline\u000a            0  \u005cnewline\u000a    \u005cend{bmatrix} \u005cnewline\u000a    y = 1 \u005cquad &if \u005cquad 5+(\u22121)x_1+0x_2\u22650 \u005cnewline\u000a    5\u2212x_1 &\u2265 0 \u005cnewline\u000a    \u2212x_1 &\u2265 \u22125 \u005cnewline\u000a    x_1 &\u2264 5 \u005cnewline\u000a\u005cend{align}\u000a</div>\u000a\u000a<p>In this case, our decision boundary is a straight vertical line placed on the graph where x1=5, and everything to the left of that denotes <span class="math">\u005c(y\u005c)</span> = 1, while everything to the right denotes <span class="math">\u005c(y\u005c)</span> = 0.</p>\u000a<p>Again, the input to the sigmoid function g(z) (e.g. <span class="math">\u005c(\u005ctheta^TX\u005c)</span>) doesn't need to be linear, and could be a function that describes a circle (e.g. <span class="math">\u005c(z=\u005ctheta_0+\u005ctheta_1x^{2}_1+\u005ctheta_2x^{2}_2\u005c)</span>) or any shape to fit our data.</p>\u000a<h3>Cost Function</h3>\u000a<p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p>\u000a<p>Instead, our cost function for logistic regression looks like:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    J(\u005ctheta) &= \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} Cost(h_\u005ctheta(x^{(i)}),y^{(i)}) \u005cnewline\u000a    Cost(h_\u005ctheta(x),y) &= \u2212log\u2061(h_\u005ctheta(x)) \u005cquad if \u005c; y = 1 \u005cnewline\u000a    Cost(h_\u005ctheta(x),y) &= \u2212log\u2061(1\u2212h_\u005ctheta(x)) \u005cquad if \u005c; y = 0 \u005cnewline\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>When y = 1, we get the following plot for <span class="math">\u005c(J(\u005ctheta)\u005c)</span> vs <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>:\u000a<div class="imgcap">\u000a<img alt="/images/assets/logisticreg/costvshypo1.png" src="/images/assets/logisticreg/costvshypo1.png" style="border:none; width:30%;">\u000a</div></p>\u000a<p>Similarly, when y = 0, we get the following plot for <span class="math">\u005c(J(\u005ctheta)\u005c)</span> vs <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>:</p>\u000a<div class="imgcap">\u000a<img alt="/images//assets/logisticreg/costvshypo2.png" src="/images//assets/logisticreg/costvshypo2.png" style="border:none; width:30%;">\u000a</div>\u000a\u000a<p>Now our cost function becomes:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    Cost(h_\u005ctheta(x),y)&=0 \u005cquad &if \u005c; h_\u005ctheta(x)=y \u005cnewline\u000a    Cost(h_\u005ctheta(x),y)&\u2192\u221e \u005cquad &if \u005c; y=0 \u005c; and \u005c; h_\u005ctheta(x)\u21921 \u005cnewline\u000a    Cost(h_\u005ctheta(x),y)&\u2192\u221e \u005cquad &if \u005c; y=1 \u005c; and \u005c; h_\u005ctheta(x)\u21920 \u005cnewline\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>If our correct answer <span class="math">\u005c(y\u005c)</span> is <span class="math">\u005c(0\u005c)</span>, then the cost function will be <span class="math">\u005c(0\u005c)</span> if our hypothesis function also outputs <span class="math">\u005c(0\u005c)</span>. If our hypothesis approaches <span class="math">\u005c(1\u005c)</span>, then the cost function will approach infinity.</p>\u000a<p>If our correct answer <span class="math">\u005c(y\u005c)</span> is <span class="math">\u005c(1\u005c)</span>, then the cost function will be <span class="math">\u005c(0\u005c)</span> if our hypothesis function outputs <span class="math">\u005c(1\u005c)</span>. If our hypothesis approaches <span class="math">\u005c(0\u005c)</span>, then the cost function will approach infinity.</p>\u000a<p>Note that writing the cost function in this way guarantees that <span class="math">\u005c(J(\u005ctheta)\u005c)</span> is convex for logistic regression.</p>\u000a<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\u000a    var align = "center",\u000a        indent = "0em",\u000a        linebreak = "false";\u000a\u000a    if (false) {\u000a        align = (screen.width < 768) ? "left" : align;\u000a        indent = (screen.width < 768) ? "0em" : indent;\u000a        linebreak = (screen.width < 768) ? 'true' : linebreak;\u000a    }\u000a\u000a    var mathjaxscript = document.createElement('script');\u000a    var location_protocol = (false) ? 'https' : document.location.protocol;\u000a    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';\u000a    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\u000a    mathjaxscript.type = 'text/javascript';\u000a    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';\u000a    mathjaxscript[(window.opera ? "innerHTML" : "text")] =\u000a        "MathJax.Hub.Config({" +\u000a        "    config: ['MMLorHTML.js']," +\u000a        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +\u000a        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +\u000a        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +\u000a        "    displayAlign: '"+ align +"'," +\u000a        "    displayIndent: '"+ indent +"'," +\u000a        "    showMathMenu: true," +\u000a        "    messageStyle: 'normal'," +\u000a        "    tex2jax: { " +\u000a        "        inlineMath: [ ['\u005c\u005c\u005c\u005c(','\u005c\u005c\u005c\u005c)'] ], " +\u000a        "        displayMath: [ ['$$','$$'] ]," +\u000a        "        processEscapes: true," +\u000a        "        preview: 'TeX'," +\u000a        "    }, " +\u000a        "    'HTML-CSS': { " +\u000a        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +\u000a        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +\u000a        "    }, " +\u000a        "}); " +\u000a        "if ('default' !== 'default') {" +\u000a            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a        "}";\u000a    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\u000a}\u000a</script>
(dp658
Vdate
p659
g5
(S'\x07\xe1\x04\x0b\x12 \x00\x00\x00\x00'
tRp660
sVcategory
p661
g8
(g9
g10
NtRp662
(dp663
g13
Vtech
p664
sg15
Vtech
p665
sg17
I01
sg18
(dp666
VIGNORE_FILES
p667
(lp668
V.#*
p669
asVPAGE_ORDER_BY
p670
Vbasename
p671
sS'ARTICLE_TWEET_BUTTON'
p672
I00
sVWITH_FUTURE_DATES
p673
I01
sVCATEGORY_SAVE_AS
p674
Vcategory/{slug}.html
p675
sS'LINKS'
p676
((VCV/Resume
Vuser
V/pages/duncan-locks-resume.html
t(VLinkedIn
Vlinkedin-squared
Vhttp://ca.linkedin.com/in/duncanlock/
ttp677
sVARTICLE_ORDER_BY
p678
Vreversed-date
p679
sVDEFAULT_LANG
p680
Ven
p681
sVCACHE_CONTENT
p682
I01
sVPAGE_LANG_SAVE_AS
p683
Vpages/{slug}-{lang}.html
p684
sVDRAFT_SAVE_AS
p685
Vdrafts/{slug}.html
p686
sVPAGE_PATHS
p687
(lp688
Vpages
p689
asVDEFAULT_PAGINATION
p690
I8
sVDOCUTILS_SETTINGS
p691
(dp692
S'math_output'
p693
S'MathJax'
p694
ssVLOAD_CONTENT_CACHE
p695
I01
sVSTATIC_PATHS
p696
(lp697
Vimages
p698
aVextras
p699
aVfiles
p700
asVDAY_ARCHIVE_SAVE_AS
p701
Vblog/{date:%Y}/{date:%m}/{date:%d}/index.html
p702
sVARTICLE_LANG_URL
p703
V{slug}-{lang}.html
p704
sVSLUGIFY_SOURCE
p705
Vtitle
p706
sVSTATIC_URL
p707
V{path}
p708
sVCATEGORY_FEED_ATOM
p709
NsVOUTPUT_SOURCES
p710
I00
sVPAGINATED_DIRECT_TEMPLATES
p711
(lp712
Vindex
p713
asVTYPOGRIFY_IGNORE_TAGS
p714
(lp715
S'.math'
p716
aS'script'
p717
asVSTATIC_EXCLUDES
p718
(lp719
sVTAG_URL
p720
Vtag/{slug}.html
p721
sVARTICLE_PATHS
p722
(lp723
Vposts
p724
asVGZIP_CACHE
p725
I00
sVFILENAME_METADATA
p726
V(?P<date>\u005cd{4}-\u005cd{2}-\u005cd{2}).*
p727
sVDEFAULT_ORPHANS
p728
I1
sVPELICAN_CLASS
p729
Vpelican.Pelican
p730
sVCACHE_PATH
p731
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blogsrc/cache
p732
sVINDEX_SAVE_AS
p733
Vindex.html
p734
sVINTRASITE_LINK_REGEX
p735
V[{|](?P<what>.*?)[|}]
p736
sVDISPLAY_PAGES_ON_MENU
p737
I00
sS'COLOPHON_TITLE'
p738
VAbout
p739
sS'ARCHIVES_SAVE_AS'
p740
Vblog/index.html
p741
sS'RESPONSIVE_IMAGES'
p742
I01
sS'AUTHOR'
p743
VRahul Pathak
p744
sS'MARKUP'
p745
(Vrst
Vhtml
tp746
sVDISPLAY_CATEGORIES_ON_MENU
p747
I01
sVCHECK_MODIFIED_METHOD
p748
Vmtime
p749
sVARTICLE_EXCLUDES
p750
(lp751
g689
asVDEFAULT_DATE_FORMAT
p752
V%a %d %B %Y
p753
sVTRANSLATION_FEED_ATOM
p754
NsVPLUGINS
p755
(lp756
Vassets
p757
aVrelated_posts
p758
aVextract_toc
p759
aVpost_stats
p760
aVseries
p761
aVrender_math
p762
aVgzip_cache
p763
aVsitemap
p764
asVTYPOGRIFY
p765
I01
sS'TRANSLATION_FEED_RSS'
p766
NsVEXTRA_TEMPLATES_PATHS
p767
(lp768
sVMARKDOWN
p769
(dp770
Vextension_configs
p771
(dp772
Vmarkdown.extensions.extra
p773
(dp774
sVmarkdown.extensions.codehilite
p775
(dp776
Vcss_class
p777
Vhighlight
p778
ssVmarkdown.extensions.meta
p779
(dp780
ssS'extensions'
p781
(lp782
g8
(g137
g10
NtRp783
(dp784
g140
I00
sbag773
ag775
ag779
asVoutput_format
p785
Vhtml5
p786
ssVOUTPUT_RETENTION
p787
(lp788
sS'COLOPHON'
p789
I01
sVPAGE_LANG_URL
p790
Vpages/{slug}-{lang}.html
p791
sS'DEFAULT_PAGESCHEMA'
p792
VBlogPost
p793
sVTHEME
p794
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blueprint
p795
sVUSE_FOLDER_AS_CATEGORY
p796
I01
sVJINJA_ENVIRONMENT
p797
(dp798
Vextensions
p799
(lp800
g159
asVlstrip_blocks
p801
I01
sVtrim_blocks
p802
I01
ssVDEBUG
p803
I00
sVREVERSE_CATEGORY_ORDER
p804
I00
sVRELATIVE_URLS
p805
I00
sVDATE_FORMATS
p806
(dp807
sVSITEURL
p808
Vhttps://summations.github.io
p809
sVCONTENT_CACHING_LAYER
p810
Vreader
p811
sS'SOCIAL'
p812
((VGitHub
Vgithub-circled
Vhttp://github.com/rahulpathakml
ttp813
sVNEWEST_FIRST_ARCHIVES
p814
I01
sS'SITE_TITLE'
p815
VRahul Pathak Blog
p816
sS'COPYRIGHT_UNTIL'
p817
I2017
sVPATH_METADATA
p818
V
sVTEMPLATE_PAGES
p819
(dp820
sVFEED_DOMAIN
p821
g809
sVDIRECT_TEMPLATES
p822
(lp823
g713
aVtags
p824
aVcategories
p825
aVauthors
p826
aVarchives
p827
asVLOG_FILTER
p828
(lp829
sS'SITE_DESCRIPTION'
p830
VRahul Pathak personal site. Includes my blog, colitis resources, SCD recipes, portfolio and CV/Resume.
p831
sVAUTHOR_FEED_RSS
p832
Vfeeds/%s.rss.xml
p833
sS'TIMEZONE'
p834
VAmerica/Vancouver
p835
sVDRAFT_URL
p836
Vdrafts/{slug}.html
p837
sVPAGE_EXCLUDES
p838
(lp839
g724
asVTAG_SAVE_AS
p840
Vtag/{slug}.html
p841
sVEXTRA_PATH_METADATA
p842
(dp843
Vextras/.htaccess
p844
(dp845
Vpath
p846
V.htaccess
p847
ssVextras/google9b8f9c7f2338fb3e.html
p848
(dp849
g846
Vgoogle9b8f9c7f2338fb3e.html
p850
ssVextras/favicon.ico
p851
(dp852
g846
Vfavicon.ico
p853
ssVextras/robots.txt
p854
(dp855
g846
Vrobots.txt
p856
sssVDEFAULT_CATEGORY
p857
Vmisc
p858
sVDRAFT_LANG_URL
p859
Vdrafts/{slug}-{lang}.html
p860
sVOUTPUT_SOURCES_EXTENSION
p861
V.text
p862
sVPAGINATION_PATTERNS
p863
(lp864
g8
(g205
g206
(I0
V{name}{number}{extension}
p865
V{name}{number}{extension}
p866
ttRp867
asVSTATIC_EXCLUDE_SOURCES
p868
I01
sVJINJA_FILTERS
p869
(dp870
Vmonth_name
p871
g214
sVtagsort
p872
g216
sVsidebar_date_format
p873
g218
sVarchive_date_format
p874
g220
sVdump
p875
g222
ssVARTICLE_SAVE_AS
p876
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/index.html
p877
sVYEAR_ARCHIVE_SAVE_AS
p878
Vblog/{date:%Y}/index.html
p879
sVCSS_FILE
p880
Vmain.css
p881
sVAUTHOR_URL
p882
Vauthor/{slug}.html
p883
sVCATEGORY_URL
p884
Vcategory/{slug}.html
p885
sS'SITEMAP'
p886
(dp887
Vchangefreqs
p888
(dp889
Varticles
p890
Vweekly
p891
sg689
Vmonthly
p892
sVindexes
p893
Vdaily
p894
ssVpriorities
p895
(dp896
g890
F0.80000000000000004
sg689
F0.5
sg893
F0.69999999999999996
ssVformat
p897
Vxml
p898
ssVSUMMARY_MAX_LENGTH
p899
I60
sVSLUG_SUBSTITUTIONS
p900
(tsS'RELATED_POSTS_MAX'
p901
I4
sVFORMATTED_FIELDS
p902
(lp903
Vsummary
p904
asVTHEME_STATIC_PATHS
p905
(lp906
Vstatic
p907
asS'CATEGORY_FEED_RSS'
p908
NsVTHEME_STATIC_DIR
p909
Vtheme
p910
sVDRAFT_LANG_SAVE_AS
p911
Vdrafts/{slug}-{lang}.html
p912
sS'FEED_ALL_RSS'
p913
NsVREADERS
p914
(dp915
sVLOCALE
p916
(lp917
V
asVWRITE_SELECTED
p918
(lp919
sVDELETE_OUTPUT_DIRECTORY
p920
I00
sVFEED_MAX_ITEMS
p921
V
sVPATH
p922
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blogsrc/content
p923
sVPAGE_URL
p924
Vpages/{slug}.html
p925
sVSITENAME
p926
VSummations
p927
sVARTICLE_PERMALINK_STRUCTURE
p928
V
sS'COLOPHON_CONTENT'
p929
V<a href="/pages/duncan-locks-resume.html">\u000aAn adaptable and enthusiastic developer with broad experience\u000aand an artistic back&shy;ground. Strong graph&shy;ical\u000acom&shy;munication, design, creative\u000a<span class="amp">&amp;</span> prob&shy;lem solving\u000askills &mdash; and an eye for detail.</a>
p930
sVARTICLE_URL
p931
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/
p932
sS'SITESCHEME'
p933
Vhttp
p934
sVMONTH_ARCHIVE_SAVE_AS
p935
Vblog/{date:%Y}/{date:%m}/index.html
p936
sVDEFAULT_METADATA
p937
(dp938
sVRSS_FEED_SUMMARY_ONLY
p939
I01
sS'ARTICLE_GOOGLEPLUS_BUTTON'
p940
I00
sVPLUGIN_PATHS
p941
(lp942
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/pelican-plugins
p943
asVFEED_ALL_ATOM
p944
NsVSTATIC_SAVE_AS
p945
g708
sVAUTHOR_SAVE_AS
p946
I00
sVPYGMENTS_RST_OPTIONS
p947
(dp948
sVAUTHOR_FEED_ATOM
p949
Vfeeds/%s.atom.xml
p950
sVARTICLE_LANG_SAVE_AS
p951
g704
sVPAGE_SAVE_AS
p952
Vpages/{slug}.html
p953
sS'OPEN_GRAPH_METADATA'
p954
I01
sVOUTPUT_PATH
p955
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blogsrc/output
p956
sVDEFAULT_STATUS
p957
Vpublished
p958
sS'DISQUS_SITENAME'
p959
Vrahulpathakml
p960
sS'COPYRIGHT_FROM'
p961
I1998
ssbsVtags
p962
(lp963
g8
(g330
g10
NtRp964
(dp965
g13
Vmachinelearning
p966
sg15
Vmachinelearning
p967
sg17
I01
sg18
g666
sbag8
(g330
g10
NtRp968
(dp969
g13
Vclassification
p970
sg15
Vclassification
p971
sg17
I01
sg18
g666
sbasVtitle
p972
VLogistic Regression
p973
sttp974
sV/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/content/posts/tech/multivariate-linearregression.md
p975
(F1492239777.8438642
(V<p>Linear regression with multiple variables is also known as "multivariate linear regression". Multiple linear regression is a generalization of linear regression by considering more than one independent variable, and a specific case of general linear models formed by restricting the number of dependent variables to one\u000aLets introduce notation for equations where we can have any number of input variables.</p>\u000a<div class="math">\u005cbegin{align*}\u000ax_j^{(i)} &amp;= \u005ctext{value of feature } j \u005ctext{ in the }i^{th}\u005ctext{ training set} \u005cnewline \u000ax^{(i)}&amp; = \u005ctext{column vector of all the features of }i^{th}\u005ctext{ training set} \u005cnewline \u000a&amp;m = \u005ctext{the number of training set} \u005cnewline \u000a&amp;n = \u005cleft| x^{(i)} \u005cright| ; \u005ctext{(the number of features)} \u000a\u005cend{align*}</div>\u000a<p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta (x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_2 + \u005ctheta_3 x_3 + \u005ccdots + \u005ctheta_n x_n\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>In order to develop intuition about this function, we can think about <span class="math">\u005c(\u005ctheta_0\u005c)</span> as the basic price of a house, <span class="math">\u005c(\u005ctheta_1\u005c)</span> as the price per square meter, <span class="math">\u005c(\u005ctheta_2\u005c)</span> as the price per floor, etc. <span class="math">\u005c(x_1\u005c)</span> will be the number of square meters in the house, <span class="math">\u005c(x_2\u005c)</span> the number of floors, etc.</p>\u000a<p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    h_\u005ctheta(x) =\u000a    \u005cbegin{bmatrix}\u000a        \u005ctheta_0 \u005chspace{2em} \u005ctheta_1 \u005chspace{2em} ... \u005chspace{2em} \u005ctheta_n\u000a    \u005cend{bmatrix}\u000a    \u005cbegin{bmatrix}\u000a        x_0 \u005cnewline \u000a        x_1 \u005cnewline \u000a        \u005cvdots \u005cnewline \u000a        x_n\u000a    \u005cend{bmatrix}= \u005ctheta^T x\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.</p>\u000a<p>Remark: Note that for convenience reasons in this course we assume <span class="math">\u005c(x_{0}^{(i)} =1 \u005ctext{ for } (i\u005cin { 1,\u005cdots, m } )\u005c)</span>. This allows us to do matrix operations with <span class="math">\u005c(\u005ctheta\u005c)</span> and <span class="math">\u005c(x\u005c)</span>. Hence making the two vectors <span class="math">\u005c(\u005ctheta\u005c)</span> and <span class="math">\u005c(x(i)\u005c)</span> match each other element-wise (that is, have the same number of elements: <span class="math">\u005c(n+1\u005c)</span>).</p>\u000a<p>The following example shows us the reason behind setting <span class="math">\u005c(x_{0}^{(i)}=1\u005c)</span> :</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    X = \u005cbegin{bmatrix}\u000a            x^{(1)}_0 & x^{(2)}_0 & x^{(3)}_0 \u005cnewline \u000a            x^{(1)}_1 & x^{(2)}_1 & x^{(3)}_1 \u000a        \u005cend{bmatrix} &,\u005ctheta = \u000a        \u005cbegin{bmatrix}\u000a            \u005ctheta_0 \u005cnewline \u000a            \u005ctheta_1 \u005cnewline\u000a        \u005cend{bmatrix}\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>As a result, you can calculate the hypothesis as a vector with <span class="math">\u005c(h_{\u03b8}(X)=\u03b8^TX\u005c)</span></p>\u000a<p>&lt;br ></p>\u000a<h4>Gradient Descent For Multiple Variables</h4>\u000a<p>The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*} \u000a\u005ctext{repeat until convergence:} \u005c; \u005clbrace \u005cnewline \u000a\u005c; & \u005ctheta_0 := \u005ctheta_0 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_0^{(i)} \u005cnewline \u000a\u005c; & \u005ctheta_1 := \u005ctheta_1 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_1^{(i)} \u005cnewline \u000a\u005c; & \u005ctheta_2 := \u005ctheta_2 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_2^{(i)} \u005cnewline \u000a& \u005ccdots \u005cnewline \u000a\u005crbrace \u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>In other words:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    \u005ctext{repeat until convergence:} \u005c; \u005clbrace \u005cnewline \u000a    \u005c; & \u005ctheta_j := \u005ctheta_j - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_j^{(i)} \u005c\u005c\u000a    \u005c; & \u005ctext{for j := 0...n}\u005cnewline \u000a    \u005crbrace\u000a\u005cend{align*}\u000a</div>\u000a\u000a<h3>Gradient Descent in Practice I - Feature Scaling</h3>\u000a<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because \u005c(\u005ctheta\u005c) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>\u000a<p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:</p>\u000a<div class="math">$$ \u22121 \u2264 x_i \u2264 1 $$</div>\u000a<p>or</p>\u000a<div class="math">$$ \u22120.5 \u2264 x_i \u2264 0.5 $$</div>\u000a<p>These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p>\u000a<p>Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    x_i & :=(x_i\u2212\u03bc_i)/s_i \u005cnewline\u000a    & s_i = \u005ctext{Standard Deviation} \u005cnewline\u000a    & \u03bc_i = \u005ctext{Mean}\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>Where \u03bci is the average of all the values for feature (i) and si is the range of values (max - min), or si is the standard deviation.</p>\u000a<p>Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.</p>\u000a<p>For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, xi:=price\u221210001900.</p>\u000a<h3>Gradient Descent in Practice II - Learning Rate</h3>\u000a<p>Note: [5:20 - the x -axis label in the right graph should be \u03b8 rather than No. of iterations ]</p>\u000a<p>Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(\u03b8) over the number of iterations of gradient descent. If J(\u03b8) ever increases, then you probably need to decrease \u03b1.</p>\u000a<p>Automatic convergence test. Declare convergence if J(\u03b8) decreases by less than E in one iteration, where E is some small value such as 10\u22123. However in practice it's difficult to choose this threshold value.</p>\u000a<div class="imgcap">\u000a<img alt="" src="/images/assets/lr-gd/costfunction-alpha1.png" style="border:none; width:50%;">\u000a</div>\u000a\u000a<p>It has been proven that if learning rate \u03b1 is sufficiently small, then \u005c( J(\u005ctheta)\u005c) will decrease on every iteration.</p>\u000a<div class="imgcap">\u000a<img alt= "" src="/images/assets/lr-gd/costfunction-alpha2.png" style="border:none; width:50%;">\u000a</div>\u000a\u000a<p>To summarize:\u000aIf \u03b1 is too small: slow convergence.\u000aIf \u03b1 is too large: \u005c( J(\u005ctheta)\u005c) may not decrease on every iteration and thus may not converge.</p>\u000a<h3>Features and Polynomial Regression</h3>\u000a<p>We can improve our features and the form of our hypothesis function in a couple different ways. We can combine multiple features into one. For example, we can combine <span class="math">\u005c(x_1\u005c)</span> and <span class="math">\u005c(x_2\u005c)</span> into a new feature <span class="math">\u005c(x_3\u005c)</span> by taking <span class="math">\u005c(x_1*x_2\u005c)</span>.</p>\u000a<h4>Polynomial Regression</h4>\u000a<p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well. We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). For example, if our hypothesis function is <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1\u005c)</span> then we can create additional features based on <span class="math">\u005c(x_1\u005c)</span> , to get the quadratic function <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_1^2\u005c)</span> or the cubic function <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_1^2 + \u005ctheta_3 x_1^3\u005c)</span>. </p>\u000a<p>In the cubic version, we have created new features <span class="math">\u005c(x_2\u005c)</span> and <span class="math">\u005c(x_3\u005c)</span> where <span class="math">\u005c(x_2=x_1^2\u005c)</span> and <span class="math">\u005c(x_3=x_1^3\u005c)</span>. To make it a square root function, we could do: <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 \u005csqrt{x_1}\u005c)</span>. One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important. eg. if <span class="math">\u005c(x_1\u005c)</span> has range 1 - 1000 then range of <span class="math">\u005c(x_1^2\u005c)</span> becomes 1 - 1000000 and that of <span class="math">\u005c(x_1^3\u005c)</span> becomes 1 - 1000000000</p>\u000a<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\u000a    var align = "center",\u000a        indent = "0em",\u000a        linebreak = "false";\u000a\u000a    if (false) {\u000a        align = (screen.width < 768) ? "left" : align;\u000a        indent = (screen.width < 768) ? "0em" : indent;\u000a        linebreak = (screen.width < 768) ? 'true' : linebreak;\u000a    }\u000a\u000a    var mathjaxscript = document.createElement('script');\u000a    var location_protocol = (false) ? 'https' : document.location.protocol;\u000a    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';\u000a    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\u000a    mathjaxscript.type = 'text/javascript';\u000a    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';\u000a    mathjaxscript[(window.opera ? "innerHTML" : "text")] =\u000a        "MathJax.Hub.Config({" +\u000a        "    config: ['MMLorHTML.js']," +\u000a        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +\u000a        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +\u000a        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +\u000a        "    displayAlign: '"+ align +"'," +\u000a        "    displayIndent: '"+ indent +"'," +\u000a        "    showMathMenu: true," +\u000a        "    messageStyle: 'normal'," +\u000a        "    tex2jax: { " +\u000a        "        inlineMath: [ ['\u005c\u005c\u005c\u005c(','\u005c\u005c\u005c\u005c)'] ], " +\u000a        "        displayMath: [ ['$$','$$'] ]," +\u000a        "        processEscapes: true," +\u000a        "        preview: 'TeX'," +\u000a        "    }, " +\u000a        "    'HTML-CSS': { " +\u000a        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +\u000a        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +\u000a        "    }, " +\u000a        "}); " +\u000a        "if ('default' !== 'default') {" +\u000a            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a        "}";\u000a    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\u000a}\u000a</script>
(dp976
Vdate
p977
g5
(S'\x07\xe1\x04\n\x12 \x00\x00\x00\x00'
tRp978
sVcategory
p979
g8
(g9
g10
NtRp980
(dp981
g13
Vtech
p982
sg15
Vtech
p983
sg17
I01
sg18
(dp984
VIGNORE_FILES
p985
(lp986
V.#*
p987
asVPAGE_ORDER_BY
p988
Vbasename
p989
sS'ARTICLE_TWEET_BUTTON'
p990
I00
sVWITH_FUTURE_DATES
p991
I01
sVCATEGORY_SAVE_AS
p992
Vcategory/{slug}.html
p993
sS'LINKS'
p994
((VCV/Resume
Vuser
V/pages/duncan-locks-resume.html
t(VLinkedIn
Vlinkedin-squared
Vhttp://ca.linkedin.com/in/duncanlock/
ttp995
sVARTICLE_ORDER_BY
p996
Vreversed-date
p997
sVDEFAULT_LANG
p998
Ven
p999
sVCACHE_CONTENT
p1000
I01
sVPAGE_LANG_SAVE_AS
p1001
Vpages/{slug}-{lang}.html
p1002
sVDRAFT_SAVE_AS
p1003
Vdrafts/{slug}.html
p1004
sVPAGE_PATHS
p1005
(lp1006
Vpages
p1007
asVDEFAULT_PAGINATION
p1008
I8
sVDOCUTILS_SETTINGS
p1009
(dp1010
S'math_output'
p1011
S'MathJax'
p1012
ssVLOAD_CONTENT_CACHE
p1013
I01
sVSTATIC_PATHS
p1014
(lp1015
Vimages
p1016
aVextras
p1017
aVfiles
p1018
asVDAY_ARCHIVE_SAVE_AS
p1019
Vblog/{date:%Y}/{date:%m}/{date:%d}/index.html
p1020
sVARTICLE_LANG_URL
p1021
V{slug}-{lang}.html
p1022
sVSLUGIFY_SOURCE
p1023
Vtitle
p1024
sVSTATIC_URL
p1025
V{path}
p1026
sVCATEGORY_FEED_ATOM
p1027
NsVOUTPUT_SOURCES
p1028
I00
sVPAGINATED_DIRECT_TEMPLATES
p1029
(lp1030
Vindex
p1031
asVTYPOGRIFY_IGNORE_TAGS
p1032
(lp1033
S'.math'
p1034
aS'script'
p1035
asVSTATIC_EXCLUDES
p1036
(lp1037
sVTAG_URL
p1038
Vtag/{slug}.html
p1039
sVARTICLE_PATHS
p1040
(lp1041
Vposts
p1042
asVGZIP_CACHE
p1043
I00
sVFILENAME_METADATA
p1044
V(?P<date>\u005cd{4}-\u005cd{2}-\u005cd{2}).*
p1045
sVDEFAULT_ORPHANS
p1046
I1
sVPELICAN_CLASS
p1047
Vpelican.Pelican
p1048
sVCACHE_PATH
p1049
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/cache
p1050
sVINDEX_SAVE_AS
p1051
Vindex.html
p1052
sVINTRASITE_LINK_REGEX
p1053
V[{|](?P<what>.*?)[|}]
p1054
sVDISPLAY_PAGES_ON_MENU
p1055
I00
sS'COLOPHON_TITLE'
p1056
VAbout
p1057
sS'ARCHIVES_SAVE_AS'
p1058
Vblog/index.html
p1059
sS'RESPONSIVE_IMAGES'
p1060
I01
sS'AUTHOR'
p1061
VRahul Pathak
p1062
sS'MARKUP'
p1063
(Vrst
Vhtml
tp1064
sVDISPLAY_CATEGORIES_ON_MENU
p1065
I01
sVCHECK_MODIFIED_METHOD
p1066
Vmtime
p1067
sVARTICLE_EXCLUDES
p1068
(lp1069
g1007
asVDEFAULT_DATE_FORMAT
p1070
V%a %d %B %Y
p1071
sVTRANSLATION_FEED_ATOM
p1072
NsVPLUGINS
p1073
(lp1074
Vassets
p1075
aVrelated_posts
p1076
aVextract_toc
p1077
aVpost_stats
p1078
aVseries
p1079
aVrender_math
p1080
aVsitemap
p1081
asVTYPOGRIFY
p1082
I01
sS'TRANSLATION_FEED_RSS'
p1083
NsVEXTRA_TEMPLATES_PATHS
p1084
(lp1085
sVMARKDOWN
p1086
(dp1087
Vextension_configs
p1088
(dp1089
Vmarkdown.extensions.extra
p1090
(dp1091
sVmarkdown.extensions.codehilite
p1092
(dp1093
Vcss_class
p1094
Vhighlight
p1095
ssVmarkdown.extensions.meta
p1096
(dp1097
ssS'extensions'
p1098
(lp1099
g8
(g137
g10
NtRp1100
(dp1101
g140
I00
sbag1090
ag1092
ag1096
asVoutput_format
p1102
Vhtml5
p1103
ssVOUTPUT_RETENTION
p1104
(lp1105
sS'COLOPHON'
p1106
I01
sVPAGE_LANG_URL
p1107
Vpages/{slug}-{lang}.html
p1108
sS'DEFAULT_PAGESCHEMA'
p1109
VBlogPost
p1110
sVTHEME
p1111
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blueprint
p1112
sVUSE_FOLDER_AS_CATEGORY
p1113
I01
sVJINJA_ENVIRONMENT
p1114
(dp1115
Vextensions
p1116
(lp1117
g159
asVlstrip_blocks
p1118
I01
sVtrim_blocks
p1119
I01
ssVDEBUG
p1120
I00
sVREVERSE_CATEGORY_ORDER
p1121
I00
sVRELATIVE_URLS
p1122
I00
sVDATE_FORMATS
p1123
(dp1124
sVSITEURL
p1125
Vhttps://summations.github.io
p1126
sVCONTENT_CACHING_LAYER
p1127
Vreader
p1128
sS'SOCIAL'
p1129
((VGitHub
Vgithub-circled
Vhttp://github.com/rahulpathakml
ttp1130
sVNEWEST_FIRST_ARCHIVES
p1131
I01
sS'SITE_TITLE'
p1132
VRahul Pathak Blog
p1133
sS'COPYRIGHT_UNTIL'
p1134
I2017
sVPATH_METADATA
p1135
V
sVTEMPLATE_PAGES
p1136
(dp1137
sVFEED_DOMAIN
p1138
g1126
sVDIRECT_TEMPLATES
p1139
(lp1140
g1031
aVtags
p1141
aVcategories
p1142
aVauthors
p1143
aVarchives
p1144
asVLOG_FILTER
p1145
(lp1146
sS'SITE_DESCRIPTION'
p1147
VRahul Pathak personal site. Includes my blog, colitis resources, SCD recipes, portfolio and CV/Resume.
p1148
sVAUTHOR_FEED_RSS
p1149
Vfeeds/%s.rss.xml
p1150
sS'TIMEZONE'
p1151
VAmerica/Vancouver
p1152
sVDRAFT_URL
p1153
Vdrafts/{slug}.html
p1154
sVPAGE_EXCLUDES
p1155
(lp1156
g1042
asVTAG_SAVE_AS
p1157
Vtag/{slug}.html
p1158
sVEXTRA_PATH_METADATA
p1159
(dp1160
Vextras/.htaccess
p1161
(dp1162
Vpath
p1163
V.htaccess
p1164
ssVextras/google9b8f9c7f2338fb3e.html
p1165
(dp1166
g1163
Vgoogle9b8f9c7f2338fb3e.html
p1167
ssVextras/favicon.ico
p1168
(dp1169
g1163
Vfavicon.ico
p1170
ssVextras/robots.txt
p1171
(dp1172
g1163
Vrobots.txt
p1173
sssVDEFAULT_CATEGORY
p1174
Vmisc
p1175
sVDRAFT_LANG_URL
p1176
Vdrafts/{slug}-{lang}.html
p1177
sVOUTPUT_SOURCES_EXTENSION
p1178
V.text
p1179
sVPAGINATION_PATTERNS
p1180
(lp1181
g8
(g205
g206
(I0
V{name}{number}{extension}
p1182
V{name}{number}{extension}
p1183
ttRp1184
asVSTATIC_EXCLUDE_SOURCES
p1185
I01
sVJINJA_FILTERS
p1186
(dp1187
Vmonth_name
p1188
g214
sVtagsort
p1189
g216
sVsidebar_date_format
p1190
g218
sVarchive_date_format
p1191
g220
sVdump
p1192
g222
ssVARTICLE_SAVE_AS
p1193
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/index.html
p1194
sVYEAR_ARCHIVE_SAVE_AS
p1195
Vblog/{date:%Y}/index.html
p1196
sVCSS_FILE
p1197
Vmain.css
p1198
sVAUTHOR_URL
p1199
Vauthor/{slug}.html
p1200
sVCATEGORY_URL
p1201
Vcategory/{slug}.html
p1202
sS'SITEMAP'
p1203
(dp1204
Vchangefreqs
p1205
(dp1206
Varticles
p1207
Vweekly
p1208
sg1007
Vmonthly
p1209
sVindexes
p1210
Vdaily
p1211
ssVpriorities
p1212
(dp1213
g1207
F0.80000000000000004
sg1007
F0.5
sg1210
F0.69999999999999996
ssVformat
p1214
Vxml
p1215
ssVSUMMARY_MAX_LENGTH
p1216
I60
sVSLUG_SUBSTITUTIONS
p1217
(tsS'RELATED_POSTS_MAX'
p1218
I4
sVFORMATTED_FIELDS
p1219
(lp1220
Vsummary
p1221
asVTHEME_STATIC_PATHS
p1222
(lp1223
Vstatic
p1224
asS'CATEGORY_FEED_RSS'
p1225
NsVTHEME_STATIC_DIR
p1226
Vtheme
p1227
sVDRAFT_LANG_SAVE_AS
p1228
Vdrafts/{slug}-{lang}.html
p1229
sS'FEED_ALL_RSS'
p1230
NsVREADERS
p1231
(dp1232
sVLOCALE
p1233
(lp1234
V
asVWRITE_SELECTED
p1235
(lp1236
sVDELETE_OUTPUT_DIRECTORY
p1237
I00
sVFEED_MAX_ITEMS
p1238
V
sVPATH
p1239
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/content
p1240
sVPAGE_URL
p1241
Vpages/{slug}.html
p1242
sVSITENAME
p1243
VSummations
p1244
sVARTICLE_PERMALINK_STRUCTURE
p1245
V
sS'COLOPHON_CONTENT'
p1246
V<a href="/pages/duncan-locks-resume.html">\u000aAn adaptable and enthusiastic developer with broad experience\u000aand an artistic back&shy;ground. Strong graph&shy;ical\u000acom&shy;munication, design, creative\u000a<span class="amp">&amp;</span> prob&shy;lem solving\u000askills &mdash; and an eye for detail.</a>
p1247
sVARTICLE_URL
p1248
Vblog/{date:%Y}/{date:%m}/{date:%d}/{slug}/
p1249
sS'SITESCHEME'
p1250
Vhttps
p1251
sVMONTH_ARCHIVE_SAVE_AS
p1252
Vblog/{date:%Y}/{date:%m}/index.html
p1253
sVDEFAULT_METADATA
p1254
(dp1255
sVRSS_FEED_SUMMARY_ONLY
p1256
I01
sS'ARTICLE_GOOGLEPLUS_BUTTON'
p1257
I00
sVPLUGIN_PATHS
p1258
(lp1259
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/pelican-plugins
p1260
asVFEED_ALL_ATOM
p1261
NsVSTATIC_SAVE_AS
p1262
g1026
sVAUTHOR_SAVE_AS
p1263
I00
sVPYGMENTS_RST_OPTIONS
p1264
(dp1265
sVAUTHOR_FEED_ATOM
p1266
Vfeeds/%s.atom.xml
p1267
sVARTICLE_LANG_SAVE_AS
p1268
g1022
sVPAGE_SAVE_AS
p1269
Vpages/{slug}.html
p1270
sS'OPEN_GRAPH_METADATA'
p1271
I01
sVOUTPUT_PATH
p1272
V/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/output
p1273
sVDEFAULT_STATUS
p1274
Vpublished
p1275
sS'DISQUS_SITENAME'
p1276
Vrahulpathakml
p1277
sS'COPYRIGHT_FROM'
p1278
I1998
ssbsVtags
p1279
(lp1280
g8
(g330
g10
NtRp1281
(dp1282
g13
Vmachinelearning
p1283
sg15
Vmachinelearning
p1284
sg17
I01
sg18
g984
sbag8
(g330
g10
NtRp1285
(dp1286
g13
Vregression
p1287
sg15
Vregression
p1288
sg17
I01
sg18
g984
sbasVtitle
p1289
VUnderstanding Multivariate Linear Regression
p1290
sttp1291
sV/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/content/posts/tech/Logistic Regression.md
p1292
(F1492255010.7084544
(V<p>To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn't work well because classification is not actually a linear function.</p>\u000a<p>The classification problem is just like the regression problem, except that the values <em>y</em> we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then <span class="math">\u005c(x^{(i)}\u005c)</span> may be some features of a piece of email, and <span class="math">\u005c(y\u005c)</span> may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, <span class="math">\u005c(y\u005cin{0,1}\u005c)</span>. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols \u201c-\u201d and \u201c+.\u201d Given x(i), the corresponding <span class="math">\u005c(y^{(i)}\u005c)</span> is also called the label for the training example. </p>\u000a<h3>Hypothesis Representation</h3>\u000a<p>We could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict <span class="math">\u005c(y\u005c)</span> given <span class="math">\u005c(x\u005c)</span>. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn\u2019t make sense for <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> to take values larger than 1 or smaller than 0 when we know that <span class="math">\u005c(y \u2208 {0, 1}\u005c)</span>. To fix this, let\u2019s change the form for our hypotheses <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> to satisfy <span class="math">\u005c(0\u2264h_\u005ctheta(x)\u22641\u005c)</span>. This is accomplished by plugging <span class="math">\u005c(\u005ctheta^Tx\u005c)</span> into the Logistic Function.</p>\u000a<p>Our new form uses the "Sigmoid Function," also called the "Logistic Function":</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)=g(\u005ctheta^Tx)z=\u005ctheta^Txg(z)=11+e\u2212z\u000a\u005cend{align*}\u000a</div>\u000a\u000a<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">loadtxt</span><span class="p">,</span> <span class="n">where</span>\u000a<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">scatter</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">legend</span><span class="p">,</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span>\u000a\u000a<span class="c1">#load the dataset</span>\u000a<span class="n">data</span> <span class="o">=</span> <span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;ex2data1.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>\u000a\u000a<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>\u000a<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>\u000a\u000a<span class="n">pos</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>\u000a<span class="n">neg</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>\u000a<span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>\u000a<span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>\u000a<span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Exam 1 score&#39;</span><span class="p">)</span>\u000a<span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Exam 2 score&#39;</span><span class="p">)</span>\u000a<span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Not Admitted&#39;</span><span class="p">,</span> <span class="s1">&#39;Admitted&#39;</span><span class="p">])</span>\u000a<span class="n">show</span><span class="p">()</span>\u000a</pre></div>\u000a\u000a\u000a<p>The following image shows us what the sigmoid function looks like:</p>\u000a<div class="figure">\u000a<img alt="" src="/images/assets/logisticreg/sigmoidfunction.png" style="border:none; width:100%;">\u000a</div>\u000a\u000a<p>The function <span class="math">\u005c(g(z)\u005c)</span>, shown here, maps any real number to the <span class="math">\u005c((0, 1)\u005c)</span> interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.</p>\u000a<p><span class="math">\u005c(h_\u005ctheta(x)\u005c)</span> will give us the probability that our output is 1. For example, <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>=0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)=P(y=1|x;\u005ctheta)=1\u2212P(y=0|x;\u005ctheta)P(y=0|x;\u005ctheta)+P(y=1|x;\u005ctheta)=1\u000a\u005cend{align*}\u000a</div>\u000a\u000a<h3>Decision Boundary</h3>\u000a<p>In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta(x)\u22650.5\u2192y=1 \u005cnewline\u000ah_\u005ctheta(x) < 0.5\u2192y=0\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5: that is when <span class="math">\u005c(g(z)\u22650.5\u005c)</span> and when <span class="math">\u005c(z\u22650\u005c)</span></p>\u000a<p>Remember.</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000az=0, e_0=1&\u21d2g(z)=1/2 \u005cnewline\u000az\u2192\u221e, e^{\u2212\u221e}\u21920&\u21d2g(z)=1 \u005cnewline\u000az\u2192\u2212\u221e, e^{\u221e}\u2192\u221e&\u21d2g(z)=0  \u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>So if our input to g is <span class="math">\u005c(\u005ctheta^TX\u005c)</span>, then that means: <span class="math">\u005c(h_\u005ctheta(x)=g(\u005ctheta^Tx)\u22650.5\u005c)</span> when <span class="math">\u005c(\u005ctheta^Tx\u22650\u005c)</span>.\u000aFrom these statements we can now say: <span class="math">\u005c(\u005ctheta^Tx\u22650\u21d2y=1\u005c)</span> and <span class="math">\u005c(\u005ctheta^Tx&lt;0\u21d2y=0\u005c)</span>.\u000aThe decision boundary is the line that separates the area where <span class="math">\u005c(y = 0\u005c)</span> and where <span class="math">\u005c(y = 1\u005c)</span>. It is created by our hypothesis function.</p>\u000a<p>Example:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align}\u000a    \u005ctheta = \u000a    &\u005cbegin{bmatrix}\u000a            5  \u005cnewline\u000a            -1 \u005cnewline\u000a            0  \u005cnewline\u000a    \u005cend{bmatrix} \u005cnewline\u000a    y = 1 \u005cquad &if \u005cquad 5+(\u22121)x_1+0x_2\u22650 \u005cnewline\u000a    5\u2212x_1 &\u2265 0 \u005cnewline\u000a    \u2212x_1 &\u2265 \u22125 \u005cnewline\u000a    x_1 &\u2264 5 \u005cnewline\u000a\u005cend{align}\u000a</div>\u000a\u000a<p>In this case, our decision boundary is a straight vertical line placed on the graph where x1=5, and everything to the left of that denotes <span class="math">\u005c(y\u005c)</span> = 1, while everything to the right denotes <span class="math">\u005c(y\u005c)</span> = 0.</p>\u000a<p>Again, the input to the sigmoid function g(z) (e.g. <span class="math">\u005c(\u005ctheta^TX\u005c)</span>) doesn't need to be linear, and could be a function that describes a circle (e.g. <span class="math">\u005c(z=\u005ctheta_0+\u005ctheta_1x^{2}_1+\u005ctheta_2x^{2}_2\u005c)</span>) or any shape to fit our data.</p>\u000a<h3>Cost Function</h3>\u000a<p>We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.</p>\u000a<p>Instead, our cost function for logistic regression looks like:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    J(\u005ctheta) &= \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} Cost(h_\u005ctheta(x^{(i)}),y^{(i)}) \u005cnewline\u000a    Cost(h_\u005ctheta(x),y) &= \u2212log\u2061(h_\u005ctheta(x)) \u005cquad if \u005c; y = 1 \u005cnewline\u000a    Cost(h_\u005ctheta(x),y) &= \u2212log\u2061(1\u2212h_\u005ctheta(x)) \u005cquad if \u005c; y = 0 \u005cnewline\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>When y = 1, we get the following plot for <span class="math">\u005c(J(\u005ctheta)\u005c)</span> vs <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>:\u000a<div class="imgcap">\u000a<img alt="/images/assets/logisticreg/costvshypo1.png" src="/images/assets/logisticreg/costvshypo1.png" style="border:none; width:30%;">\u000a</div></p>\u000a<p>Similarly, when y = 0, we get the following plot for <span class="math">\u005c(J(\u005ctheta)\u005c)</span> vs <span class="math">\u005c(h_\u005ctheta(x)\u005c)</span>:</p>\u000a<div class="imgcap">\u000a<img alt="/images//assets/logisticreg/costvshypo2.png" src="/images//assets/logisticreg/costvshypo2.png" style="border:none; width:30%;">\u000a</div>\u000a\u000a<p>Now our cost function becomes:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    Cost(h_\u005ctheta(x),y)&=0 \u005cquad &if \u005c; h_\u005ctheta(x)=y \u005cnewline\u000a    Cost(h_\u005ctheta(x),y)&\u2192\u221e \u005cquad &if \u005c; y=0 \u005c; and \u005c; h_\u005ctheta(x)\u21921 \u005cnewline\u000a    Cost(h_\u005ctheta(x),y)&\u2192\u221e \u005cquad &if \u005c; y=1 \u005c; and \u005c; h_\u005ctheta(x)\u21920 \u005cnewline\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>If our correct answer <span class="math">\u005c(y\u005c)</span> is <span class="math">\u005c(0\u005c)</span>, then the cost function will be <span class="math">\u005c(0\u005c)</span> if our hypothesis function also outputs <span class="math">\u005c(0\u005c)</span>. If our hypothesis approaches <span class="math">\u005c(1\u005c)</span>, then the cost function will approach infinity.</p>\u000a<p>If our correct answer <span class="math">\u005c(y\u005c)</span> is <span class="math">\u005c(1\u005c)</span>, then the cost function will be <span class="math">\u005c(0\u005c)</span> if our hypothesis function outputs <span class="math">\u005c(1\u005c)</span>. If our hypothesis approaches <span class="math">\u005c(0\u005c)</span>, then the cost function will approach infinity.</p>\u000a<p>Note that writing the cost function in this way guarantees that <span class="math">\u005c(J(\u005ctheta)\u005c)</span> is convex for logistic regression.</p>\u000a<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\u000a    var align = "center",\u000a        indent = "0em",\u000a        linebreak = "false";\u000a\u000a    if (false) {\u000a        align = (screen.width < 768) ? "left" : align;\u000a        indent = (screen.width < 768) ? "0em" : indent;\u000a        linebreak = (screen.width < 768) ? 'true' : linebreak;\u000a    }\u000a\u000a    var mathjaxscript = document.createElement('script');\u000a    var location_protocol = (false) ? 'https' : document.location.protocol;\u000a    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';\u000a    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\u000a    mathjaxscript.type = 'text/javascript';\u000a    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';\u000a    mathjaxscript[(window.opera ? "innerHTML" : "text")] =\u000a        "MathJax.Hub.Config({" +\u000a        "    config: ['MMLorHTML.js']," +\u000a        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +\u000a        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +\u000a        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +\u000a        "    displayAlign: '"+ align +"'," +\u000a        "    displayIndent: '"+ indent +"'," +\u000a        "    showMathMenu: true," +\u000a        "    messageStyle: 'normal'," +\u000a        "    tex2jax: { " +\u000a        "        inlineMath: [ ['\u005c\u005c\u005c\u005c(','\u005c\u005c\u005c\u005c)'] ], " +\u000a        "        displayMath: [ ['$$','$$'] ]," +\u000a        "        processEscapes: true," +\u000a        "        preview: 'TeX'," +\u000a        "    }, " +\u000a        "    'HTML-CSS': { " +\u000a        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +\u000a        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +\u000a        "    }, " +\u000a        "}); " +\u000a        "if ('default' !== 'default') {" +\u000a            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a        "}";\u000a    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\u000a}\u000a</script>
(dp1293
Vdate
p1294
g5
(S'\x07\xe1\x04\x0b\x12 \x00\x00\x00\x00'
tRp1295
sVcategory
p1296
g8
(g9
g10
NtRp1297
(dp1298
g13
Vtech
p1299
sg15
Vtech
p1300
sg17
I01
sg18
g984
sbsVtags
p1301
(lp1302
g8
(g330
g10
NtRp1303
(dp1304
g13
Vmachinelearning
p1305
sg15
Vmachinelearning
p1306
sg17
I01
sg18
g984
sbag8
(g330
g10
NtRp1307
(dp1308
g13
Vclassification
p1309
sg15
Vclassification
p1310
sg17
I01
sg18
g984
sbasVtitle
p1311
VLogistic Regression
p1312
sttp1313
sV/home/rpathak/Temporary/tmp_scratchpad/pelican_themes/summations.github.io/blogsrc/content/posts/tech/multivariate-linearregression.md
p1314
(F1492239777.8438642
(V<p>Linear regression with multiple variables is also known as "multivariate linear regression". Multiple linear regression is a generalization of linear regression by considering more than one independent variable, and a specific case of general linear models formed by restricting the number of dependent variables to one\u000aLets introduce notation for equations where we can have any number of input variables.</p>\u000a<div class="math">\u005cbegin{align*}\u000ax_j^{(i)} &amp;= \u005ctext{value of feature } j \u005ctext{ in the }i^{th}\u005ctext{ training set} \u005cnewline \u000ax^{(i)}&amp; = \u005ctext{column vector of all the features of }i^{th}\u005ctext{ training set} \u005cnewline \u000a&amp;m = \u005ctext{the number of training set} \u005cnewline \u000a&amp;n = \u005cleft| x^{(i)} \u005cright| ; \u005ctext{(the number of features)} \u000a\u005cend{align*}</div>\u000a<p>The multivariable form of the hypothesis function accommodating these multiple features is as follows:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000ah_\u005ctheta (x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_2 + \u005ctheta_3 x_3 + \u005ccdots + \u005ctheta_n x_n\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>In order to develop intuition about this function, we can think about <span class="math">\u005c(\u005ctheta_0\u005c)</span> as the basic price of a house, <span class="math">\u005c(\u005ctheta_1\u005c)</span> as the price per square meter, <span class="math">\u005c(\u005ctheta_2\u005c)</span> as the price per floor, etc. <span class="math">\u005c(x_1\u005c)</span> will be the number of square meters in the house, <span class="math">\u005c(x_2\u005c)</span> the number of floors, etc.</p>\u000a<p>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    h_\u005ctheta(x) =\u000a    \u005cbegin{bmatrix}\u000a        \u005ctheta_0 \u005chspace{2em} \u005ctheta_1 \u005chspace{2em} ... \u005chspace{2em} \u005ctheta_n\u000a    \u005cend{bmatrix}\u000a    \u005cbegin{bmatrix}\u000a        x_0 \u005cnewline \u000a        x_1 \u005cnewline \u000a        \u005cvdots \u005cnewline \u000a        x_n\u000a    \u005cend{bmatrix}= \u005ctheta^T x\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.</p>\u000a<p>Remark: Note that for convenience reasons in this course we assume <span class="math">\u005c(x_{0}^{(i)} =1 \u005ctext{ for } (i\u005cin { 1,\u005cdots, m } )\u005c)</span>. This allows us to do matrix operations with <span class="math">\u005c(\u005ctheta\u005c)</span> and <span class="math">\u005c(x\u005c)</span>. Hence making the two vectors <span class="math">\u005c(\u005ctheta\u005c)</span> and <span class="math">\u005c(x(i)\u005c)</span> match each other element-wise (that is, have the same number of elements: <span class="math">\u005c(n+1\u005c)</span>).</p>\u000a<p>The following example shows us the reason behind setting <span class="math">\u005c(x_{0}^{(i)}=1\u005c)</span> :</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    X = \u005cbegin{bmatrix}\u000a            x^{(1)}_0 & x^{(2)}_0 & x^{(3)}_0 \u005cnewline \u000a            x^{(1)}_1 & x^{(2)}_1 & x^{(3)}_1 \u000a        \u005cend{bmatrix} &,\u005ctheta = \u000a        \u005cbegin{bmatrix}\u000a            \u005ctheta_0 \u005cnewline \u000a            \u005ctheta_1 \u005cnewline\u000a        \u005cend{bmatrix}\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>As a result, you can calculate the hypothesis as a vector with <span class="math">\u005c(h_{\u03b8}(X)=\u03b8^TX\u005c)</span></p>\u000a<p>&lt;br ></p>\u000a<h4>Gradient Descent For Multiple Variables</h4>\u000a<p>The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*} \u000a\u005ctext{repeat until convergence:} \u005c; \u005clbrace \u005cnewline \u000a\u005c; & \u005ctheta_0 := \u005ctheta_0 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_0^{(i)} \u005cnewline \u000a\u005c; & \u005ctheta_1 := \u005ctheta_1 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_1^{(i)} \u005cnewline \u000a\u005c; & \u005ctheta_2 := \u005ctheta_2 - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_2^{(i)} \u005cnewline \u000a& \u005ccdots \u005cnewline \u000a\u005crbrace \u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>In other words:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    \u005ctext{repeat until convergence:} \u005c; \u005clbrace \u005cnewline \u000a    \u005c; & \u005ctheta_j := \u005ctheta_j - \u005calpha \u005cfrac{1}{m} \u005csum\u005climits_{i=1}^{m} (h_\u005ctheta(x^{(i)}) - y^{(i)}) \u005ccdot x_j^{(i)} \u005c\u005c\u000a    \u005c; & \u005ctext{for j := 0...n}\u005cnewline \u000a    \u005crbrace\u000a\u005cend{align*}\u000a</div>\u000a\u000a<h3>Gradient Descent in Practice I - Feature Scaling</h3>\u000a<p>We can speed up gradient descent by having each of our input values in roughly the same range. This is because \u005c(\u005ctheta\u005c) will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.</p>\u000a<p>The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:</p>\u000a<div class="math">$$ \u22121 \u2264 x_i \u2264 1 $$</div>\u000a<p>or</p>\u000a<div class="math">$$ \u22120.5 \u2264 x_i \u2264 0.5 $$</div>\u000a<p>These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.</p>\u000a<p>Two techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:</p>\u000a<div class="mathsize">\u000a\u005cbegin{align*}\u000a    x_i & :=(x_i\u2212\u03bc_i)/s_i \u005cnewline\u000a    & s_i = \u005ctext{Standard Deviation} \u005cnewline\u000a    & \u03bc_i = \u005ctext{Mean}\u000a\u005cend{align*}\u000a</div>\u000a\u000a<p>Where \u03bci is the average of all the values for feature (i) and si is the range of values (max - min), or si is the standard deviation.</p>\u000a<p>Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.</p>\u000a<p>For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, xi:=price\u221210001900.</p>\u000a<h3>Gradient Descent in Practice II - Learning Rate</h3>\u000a<p>Note: [5:20 - the x -axis label in the right graph should be \u03b8 rather than No. of iterations ]</p>\u000a<p>Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(\u03b8) over the number of iterations of gradient descent. If J(\u03b8) ever increases, then you probably need to decrease \u03b1.</p>\u000a<p>Automatic convergence test. Declare convergence if J(\u03b8) decreases by less than E in one iteration, where E is some small value such as 10\u22123. However in practice it's difficult to choose this threshold value.</p>\u000a<div class="imgcap">\u000a<img alt="" src="/images/assets/lr-gd/costfunction-alpha1.png" style="border:none; width:50%;">\u000a</div>\u000a\u000a<p>It has been proven that if learning rate \u03b1 is sufficiently small, then \u005c( J(\u005ctheta)\u005c) will decrease on every iteration.</p>\u000a<div class="imgcap">\u000a<img alt= "" src="/images/assets/lr-gd/costfunction-alpha2.png" style="border:none; width:50%;">\u000a</div>\u000a\u000a<p>To summarize:\u000aIf \u03b1 is too small: slow convergence.\u000aIf \u03b1 is too large: \u005c( J(\u005ctheta)\u005c) may not decrease on every iteration and thus may not converge.</p>\u000a<h3>Features and Polynomial Regression</h3>\u000a<p>We can improve our features and the form of our hypothesis function in a couple different ways. We can combine multiple features into one. For example, we can combine <span class="math">\u005c(x_1\u005c)</span> and <span class="math">\u005c(x_2\u005c)</span> into a new feature <span class="math">\u005c(x_3\u005c)</span> by taking <span class="math">\u005c(x_1*x_2\u005c)</span>.</p>\u000a<h4>Polynomial Regression</h4>\u000a<p>Our hypothesis function need not be linear (a straight line) if that does not fit the data well. We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). For example, if our hypothesis function is <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1\u005c)</span> then we can create additional features based on <span class="math">\u005c(x_1\u005c)</span> , to get the quadratic function <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_1^2\u005c)</span> or the cubic function <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 x_1^2 + \u005ctheta_3 x_1^3\u005c)</span>. </p>\u000a<p>In the cubic version, we have created new features <span class="math">\u005c(x_2\u005c)</span> and <span class="math">\u005c(x_3\u005c)</span> where <span class="math">\u005c(x_2=x_1^2\u005c)</span> and <span class="math">\u005c(x_3=x_1^3\u005c)</span>. To make it a square root function, we could do: <span class="math">\u005c(h_{\u005ctheta}(x) = \u005ctheta_0 + \u005ctheta_1 x_1 + \u005ctheta_2 \u005csqrt{x_1}\u005c)</span>. One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important. eg. if <span class="math">\u005c(x_1\u005c)</span> has range 1 - 1000 then range of <span class="math">\u005c(x_1^2\u005c)</span> becomes 1 - 1000000 and that of <span class="math">\u005c(x_1^3\u005c)</span> becomes 1 - 1000000000</p>\u000a<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {\u000a    var align = "center",\u000a        indent = "0em",\u000a        linebreak = "false";\u000a\u000a    if (false) {\u000a        align = (screen.width < 768) ? "left" : align;\u000a        indent = (screen.width < 768) ? "0em" : indent;\u000a        linebreak = (screen.width < 768) ? 'true' : linebreak;\u000a    }\u000a\u000a    var mathjaxscript = document.createElement('script');\u000a    var location_protocol = (false) ? 'https' : document.location.protocol;\u000a    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';\u000a    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';\u000a    mathjaxscript.type = 'text/javascript';\u000a    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';\u000a    mathjaxscript[(window.opera ? "innerHTML" : "text")] =\u000a        "MathJax.Hub.Config({" +\u000a        "    config: ['MMLorHTML.js']," +\u000a        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +\u000a        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +\u000a        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +\u000a        "    displayAlign: '"+ align +"'," +\u000a        "    displayIndent: '"+ indent +"'," +\u000a        "    showMathMenu: true," +\u000a        "    messageStyle: 'normal'," +\u000a        "    tex2jax: { " +\u000a        "        inlineMath: [ ['\u005c\u005c\u005c\u005c(','\u005c\u005c\u005c\u005c)'] ], " +\u000a        "        displayMath: [ ['$$','$$'] ]," +\u000a        "        processEscapes: true," +\u000a        "        preview: 'TeX'," +\u000a        "    }, " +\u000a        "    'HTML-CSS': { " +\u000a        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +\u000a        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +\u000a        "    }, " +\u000a        "}); " +\u000a        "if ('default' !== 'default') {" +\u000a            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +\u000a                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +\u000a                "VARIANT['normal'].fonts.unshift('MathJax_default');" +\u000a                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +\u000a                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +\u000a                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +\u000a            "});" +\u000a        "}";\u000a    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);\u000a}\u000a</script>
(dp1315
Vdate
p1316
g5
(S'\x07\xe1\x04\n\x12 \x00\x00\x00\x00'
tRp1317
sVcategory
p1318
g8
(g9
g10
NtRp1319
(dp1320
g13
Vtech
p1321
sg15
Vtech
p1322
sg17
I01
sg18
g666
sbsVtags
p1323
(lp1324
g8
(g330
g10
NtRp1325
(dp1326
g13
Vmachinelearning
p1327
sg15
Vmachinelearning
p1328
sg17
I01
sg18
g666
sbag8
(g330
g10
NtRp1329
(dp1330
g13
Vregression
p1331
sg15
Vregression
p1332
sg17
I01
sg18
g666
sbasVtitle
p1333
VUnderstanding Multivariate Linear Regression
p1334
sttp1335
s.